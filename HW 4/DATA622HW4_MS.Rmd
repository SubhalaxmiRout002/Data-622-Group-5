---
title: "DATA622_HW4"
author: "Group 5"
date: "`r Sys.Date()`" # Due 5/7/2021
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
#libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(inspectdf)
library(corrplot)
library(stats)
library(kableExtra)
library(factoextra)
library(kableExtra)
```

## Authorship

**Group 5:** 

* Don (Geeth) Padmaperuma,
* Subhalaxmi Rout, 
* Isabel R., and
* Magnus Skonberg


# Background

The purpose of this assignment was to explore Clustering, Principal Component Analysis, and Support Vector Machines.

*Clustering* is used as a means of grouping a set of observations in a way that those in the same group are more similar to one another than those in other groups. *Principal Component Analysis* is used to a means of variable reduction where it's our aim to preserve as much useful information as possible. In short, we trade a little accuracy for simplicity. *Support Vector Machines*, on another note, are a supervised learning technique used for classification and regression analysis. We're going to apply it as a means of classification based on one of the variables in our set (suicide).

## Our Approach

We'll perform a relatively in-depth exploratory data analysis (EDA) to better familiarize ourselves with the data at hand and then we'll explore each of the aforementioned methods.

Grouping our patients via k-means / hierarchical clustering and profiling these groups based on shared characteristics, focusing upon a subset of our data (ie. ADHD or mood disorder) in applying Principal Component Analysis, and applying a Support Vector Machine model based on whether or not a patient has attempted suicide.


................................................................................

# The Data

The data at hand is based on a real life research project and deals with a number of measures of mental health and well-being - attention deficit hyperactivity disorder (ADHD), mood / mental disorder (MD), substance abuse, etc. 

Being that our exploration and application of methods can take on many forms and head in a nearly infinite number of directions, the room for creativity is without bounds.

We start by loading in our data and utilizing built-in head() and dim() functions for an early read.
```{r}
#Load in data
#data <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/HW%204/ADHD_data.csv")
data <- read.csv("ADHD_data.csv")
#head(data)
#dim(data)
```

## Exploratory Data Analysis

From our initial read of our dataset's dimensions and first 6 entries, we observe that we're dealing with a 175 observation x 54 variable dataset. Our initial dataset is broad and may be difficult to manage / make sense of. With the motivation of clarification in mind, we thus break our larger dataset into subsets:

1. **genl**: all identifier variables (race, sex, education), total score variables, and history-related variables.
2. **adhd**: all adhd related question and total score variables.
3. **md**: all mental disorder related question and total score variables.
4. **sub**: all substance abuse related question and total score variables.
5. **hstry**: all violence, abuse, and substance history related variables.

We may end up merging these subsets, pulling individual variables, or exploring individual subsets later but this provides a means of effectively recognizing where related variables lie while simultaneously providing smaller, more manageable dataframes to explore and work upon.

Once these dataframes are created, we see that **adhd** and **md** have total score variables whereas **sub** does not, even though its data (across variables) maintains a consistent scoring system. Thus, we take it upon ourselves to add this variable for sake of consistency:

```{r}
genl <- data[c(1:4,23,39)]
#head(genl) #drop initial, add sub and hstry (all vars or "total score")
#we may want to use these separately (ie. for Q2)
adhd <- data[c(5:23)]
md <- data[c(24:39)]
sub <- data[c(40:45)]
hstry <- data[c(46:54)]
sub$SUB.Total <- rowSums(sub) #add Total column to sub
#Finalize genl dataframe (for EDA)
genl$SUB.Total <- sub$SUB.Total #add SUB.Total to genl
genl <- genl[c(-1)] #drop Initial
genl <- cbind(genl, hstry) #add history variables back in with general
head(genl)
```

We subset our dataframes (as noted above), add a "total" column for **sub**, and finalize our **genl** dataframe by merging the total column for **sub**, dropping the `Initial` variable, and adding all **hstry** variables back in.

We lean on the data dictionary (provided with our .xlsx file) to outline the variables at hand:

* `Age`: quantitative variable representative of the individual's age.
* `Sex`: categorical variable representative of the individual's sex (male-1, female-2).
* `Race`: categorical variable representative of the individual's race (white-1, african american-2, hispanic-3, asian-4, native american-5, other / missing data-6).
* `ADHD.Total`: quantitative variable representative of the individual's total self-report score for ADHD scale. It's the cumulative score for 18 questions with a scoring metric of never-0, rarely-1, sometimes-2, often-3, and very often-4.
* `MD.TOTAL`: quantitative variable representative of the individual's total self-report score for mental disorder questions. It's the cumulative score for 15 questions with a scoring metric of no-0, yes-1; question 3: no problem-0, minor-1, moderate-2, and serious-3.
* `SUB.Total`: quantitative variable representative of the individual's total self-report score for substance abuse related questions. It's the cumulative score across 6 categories with a scoring metric of no use-0, use-1, abuse-2, and dependence-3.
* `Court.order`: categorical variable representative of whether the individual's case was court ordered (No-0, Yes-1).
* `Education`: categorical variable representative of the individual's level of education (1-12 grade, 13+ college).
* `Hx.of.Violence`: categorical variable representative of whether the individual has a history of violence (No-0, Yes-1).
* `Disorderly.Conduct`: categorical variable representative of whether the individual has a record of disorderly conduct (No-0, Yes-1).
* `Suicide`: categorical variable representative of whether the individual has attempted suicide in the past (No-0, Yes-1).
* `Abuse`: categorical variable representative of whether the individual has a history of abuse (No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7).
* `Non.subst.Dx`: categorical variable representative of whether the individual has a non-substance diagonosis (0 – none; 1 – one; 2 – More than one).
* `Subst.Dx`: categorical variable representative of whether the individual has a substance diagonosis (0 – none; 1 – one Substance-related; 2 – two; 3 – three or more).
* `Psych.meds.`: categorical variable representative of whether the individual has been prescribed psychiatric medication (0 – none; 1 – one psychotropic med; 2 – more than one psychotropic med).

The resulting dataframe (and variables) are shown above and at this point we're prepared to more deeply explore our **genl** dataset.

To do so, we utilize the built-in summary() and glimpse() functions:

```{r}
summary(genl)
glimpse(genl)
```

From the above outputs we can extend that:

* the average `Age` of respondents is ~40, there are slightly more male than female respondents, and the average education level ~12 (finishing high school).
* the average `ADHD,Total` was ~34, the average `MD.TOTAL` was ~10, and the averge `SUB.Total` was ~4.
* `Disorderly.Conduct` appears to be more common than `Hx.of.Violence` which in turn is more common than `Court.order`.
* approximately 1/3 of respondents have attempted `Suicide` and some form of abuse appears to be common (ie. Physical).
* the average `Non.subst.Dx`  was ~0.4 while the average `Subst.Dx` was ~1.1 giving the impression that we're dealing with more substance diagnoses that non-substance diagnoses.
* `Psych.meds` is missing the majority of its values and NA values will have to be accounted for (in general).

Based on this information, we drop `Psych.meds`, drop remaining NA values, and observe the affect on our dataset:

```{r}
genl <- genl[c(-15)] #drop Psych.meds
genl <- genl %>% tibble() %>% drop_na() #drop NA values
#revisit genl
dim(genl) #175 - 142 = 33 dropped rows
colSums(is.na(genl)) #verify no NAs
```

Our dataset has been reduced from 175 observations x 15 variables to 142 observations x 14 variables and from numerous NA values across multiple variables to no NA values.

With NA values dealt with, we ensure each variable's of the proper type and then visualize our variable distributions based on whether they're categorical or numeric: 

```{r}
#convert features to factor
genl$Sex <- as.factor(genl$Sex)
genl$Race <- as.factor(genl$Race)
genl$Court.order <- as.factor(genl$Court.order)
genl$Education <- as.factor(genl$Education)
genl$Hx.of.Violence <- as.factor(genl$Hx.of.Violence)
genl$Disorderly.Conduct <- as.factor(genl$Disorderly.Conduct)
genl$Suicide <- as.factor(genl$Suicide)
genl$Abuse <- as.factor(genl$Abuse)
genl$Non.subst.Dx <- as.factor(genl$Non.subst.Dx)
genl$Subst.Dx <- as.factor(genl$Subst.Dx)

#convert features to numeric
genl$Age <- as.numeric(genl$Age)
genl$ADHD.Total <- as.numeric(genl$ADHD.Total)
genl$MD.TOTAL <- as.numeric(genl$MD.TOTAL)
genl$SUB.Total <- as.numeric(genl$SUB.Total)
#head(genl) #verify conversions

#visualize categorical distributions as a table
fig1 <- inspectdf::inspect_imb(genl)
fig1
```

From the categorical variable table above we see that ~92% of respondent's cases were not court ordered, ~74% of respondents do not have a history of violence, ~70% of respondents have a history of disorderly conduct, ~68% of respondents have not attempted suicide, ~65% of respondents do not have a non-substance diagnosis, ~63% of respondents do not have a history of abuse, ~56% of respondents are African American, ~54% of respondents are male, ~39% of respondents have completed *at least* high school, and ~39% of respondents have one substance related diagnosis. *The fact that `Education` and `Subst.Dx` have the same pcnt values is interesting and worth noting.*

Moving on to our numeric distributions:

```{r}
#visualize numeric distributions 
fig2 <- inspectdf::inspect_num(genl) %>% 
    show_plot()
fig2
```

From the histograms above we see that:

* `ADHD.Total` has a left skewed normal distribution with a peak ~45.
* `Age` appears to be multimodal with peaks ~25, ~35 and ~50.
* `MD.TOTAL` has a non-normal distribution with a peak concentrated > 15 (at the far right).
* `SUB.Total` appears to have a multi-modal distribution with its largest peak at ~3.

With insight gained via variable distributions, we move on to observing the pair plot and correlation matrix for numeric variables:

```{r}
#paired plot of numeric variables
pairs(genl %>% select_if(is.numeric))
#Correlation matrix for numeric variables
genl_corr <- genl %>%
    select_if(is.numeric) %>%
    cor()
corrplot(genl_corr, title="Numeric Variable Correlation",type = "lower", mar=c(0,0,1,0))
```

The pair plot and correlation matrix highlight the fact that `ADHD.Total` and `MD.TOTAL` appear to have strong correlation. We'll note this fact, while carrying both variables forward.


................................................................................

# 1. Clustering

We elected to utilize **hierarchical clustering** being that it's more adaptive than k-means clustering. For k-means clustering, we have to pre-define our k. Whereas for hierarchical clustering both the value of k and the location of our centroids (mid-point between all data points in the group) is determined as a part of the process.

We proceed by computing the distances between all data points (using Euclidean distance), clustering based on `distances` based on centroid distance *and* the variance in each cluster, and then plotting the corresponding dendrogram:

```{r}
set.seed(123)
#compute distances between all data points (using euclidean distance)
distances <- dist(genl, method = 'euclidean')
#cluster based on distances using centroid distance as well as variance in clusters
clusterHealth <- hclust(distances, method = 'ward.D') #ward.D accounts for centroid distance + variance
#plot the dendrogram
plot(clusterHealth)
```

Based on our dendrogram, it appears that the ideal number of clusters is either 3, 4, or 5. **We proceed with 5 clusters** being that it's the higher of the three and we'd like to better represent the variation we observed during our exploratory data analysis. We want to better capture the distinct characteristics of our clustered groups.

We "cut the tree" and demarcate our groups based on the desired number of clusters, calculate each numeric variables' mean value for each cluster, and then output the resulting table:

```{r}
clusterGroups <- cutree(clusterHealth, 5)
#clusterGroups #verify the breakdown per observation
#Mean value for numeric variables
#tapply(genl$Age, clusterGroups, mean, simplify=TRUE) #avg age for each cluster
#tapply(genl$ADHD.Total, clusterGroups, mean) #avg ADHD score for each cluster
#tapply(genl$MD.TOTAL, clusterGroups, mean) #avg MD score for each cluster
#tapply(genl$SUB.Total, clusterGroups, mean) #avg SUB score for each cluster
#Extract corresponding values (to place in easy to understand form):
avg_age <- c(27.06, 48.95, 41.42, 47.4, 25.22)
avg_adhd <- c(49.34, 53.52, 34.12, 15.93, 6.56)
avg_md <- c(12.84, 12.71, 10.58, 7.27, 4.11)
avg_sub <- c(4.13, 3.28, 4.1, 4.17, 3.44)
#Build kable table
group_names <- c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5")
df <- data.frame(group_names, avg_age, avg_adhd, avg_md, avg_sub)
df %>% 
    kbl() %>%
    kable_styling(latex_options = "striped")
```

Based on the output above we'd profile our groups as follows:

* Group 1: **Young and Distressed**. This is the 2nd youngest group (late 20s) with the 2nd highest incidence of ADHD and the highest incidence of mental disorder.
* Group 2: **Old and Unfocused**. This is the oldest group (late 40s) with the highest incidence of ADHD and the 2nd highest incidence of mental disorder.
* Group 3: **Mid Age and Mid Tiered**. This group is "middle of the run". They're toward their middle years (early 40s), and have moderate incidence of ADHD and mental disorder.
* Group 4: **Old and Stable**. This is the 2nd most stable group. They're in their  late 40s and have low incidence of ADHD and mental disorder.
* Group 5: **Young and Driven**. To contrast the **Young and Distressed** group, this group is young and stable. This is the most stable group. They're in their mid 20s and have the lowest incidence of ADHD and mental disorder.

It was interesting to profile our groups and observe that there appeared to be 2 tiers in the earlier and later years. It was also interesting to see that all groups had a relatively low incidence of substance use / dependency. Due to its apparent low impact as a differentiating factor for our clusters it was not discussed.


................................................................................

# 2. Principal Component Analysis

Principal Component Analysis (PCA) is a way of reducing the dimensions of a given dataset by extracting new features from the original features present in the dataset. The “new” variables after PCA are all independent of one another. We elected to perform Principal Component Analysis on the ADHD self-report columns, MD (mood disorder) questionnaire, and entire dataset.

## ADHD

We start with the Attention Deficit Hyperactivity Disorder (ADHD) symptom variables. During EDA, we'd subset datasets based on question type and thus we proceed with the `adhd` dataset.

There are 19 variables in the `adhd` dataset, 18 pertaining to self-reported ADHD question responses and 1 the total score for that row. All ADHD questions capture categorical responses: never-0, rarely-1, sometimes-2, often-3, and very often-4 whereas the `ADHD.Total` is a quantitative (sum) variable representative of the individual's total self-report score for ADHD scale.

We verify that our data does not have missing values:

```{r}
# Check for NAs
colSums(is.na(adhd))

```

Missing values would disturb the implementation of PCA (where we'd have to drop / impute corresponding values), and we confirm that our data has 0 missing values across all variables.

We then perform PCA via **pcrcomp()** function and review corresponding statistics: 

```{r}
# Perform PCA and review summary statistics
pca_adhd <- prcomp(adhd)
summary(pca_adhd)

```

From the `Importance of components`, we can interpret:

* `Standard deviation` as a measure of variability across a single component (ie. deviation of data within `PC1`)
* `Proportion of Variance` as a measure of the component's variability / representative capability for the entire dataset (ie. the sum of the `Proportion of Variance` row = 1.00).
* `Cumulative Proportion` as a measure of the cumulative variability represented by principal components up until the component column we're on.

From these definitions we can extend that the first 3 components (`PC1`, `PC2`, and `PC3`) represent ~ 96.5% of the data and remaining components add but minor increments to this representative capability.

Next, we visit the scree plot:

```{r}
# scree plot
plot(pca_adhd, type = 'l')
```

If we recall from statistics: $var = sd^2$, where $var$ represents variance and $sd$ represents standard deviation.

From the scree plot above, we confirm our earlier finding that **PC1, PC2, and PC3 appear to be enough to represent our larger dataset.**

### Correlation between variables and principal components:

To explore how our variables (original and PCA) relate to one another, we bind `PC1`, `PC2`, and `PC3` to our original `adhd` dataset, calculate variable correlation and output the result as a table:

```{r}
# combine datasets
adhd_df2 <- cbind(adhd, pca_adhd$x)
# %>% column_spec(which(ADHD_df_2[,19] > 0.69) , color = 'white', background = 'maroon')

# cor plot
cor(adhd, adhd_df2[, 20:22]) %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down") 

```

From the correlation table we can extend that:

* All variables have a strong positive correlation with `PC1` *and* it appears that `PC1` is identical to `ADHD.Total` based on their correlation of ~1.00.
* Q1: Q5, Q7, Q8, and Q11 have a negative correlation, Q9, Q10, Q13, and Q14 have a *slight* negative correlation, Q12, Q15:Q18 have a positive correlation, and Q6 and `ADHD.Total` have a *slight* positive correlation with `PC2`.
* Q1:Q4, Q8:Q10, Q12 and Q16 have a positive correlation, Q7, Q17, and Q18 have a *slight* positive correlation, Q5, Q6, Q13 and Q14 have a negative correlation, and Q11, Q15, and `ADHD.Total` have a *slight* negative correlation with with `PC3`.

The correlations above provide indication as to the makeup of our principal components. Those with a stronger positive correlation are more a component while those with a stronger negative correlation are less a component.

From our correlation table, we move on to the corresponding biplot.

### Bi-plots

When our first two PCs explain a significant portion of the variance in our data (which is the case for our application), we can make use of a bi-plot. 

A bi-plot overlays the scores plot, a projection of observations onto the span of the first two PCs, and a loadings plot, a projection of the variable vectors onto the span of the PCs, in a single graphic:

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC1 and PC2
biplot(pca_adhd, choices = c(1,2), scale = 0)
```

Points are projected observations while vectors are projected variables. In the above plot, the red arrows represent the eigenvectors of each variable helping us to interpret the relationship between principal components while the numbers in black represent our observations. 

With regard to reading the plot, the left and bottom axes are used to read PCA scores (of the dots) while the top and right axes are used to read how strongly each vector influences the PC.

From the above biplot, we can extend that:
* the observations appear to play a larger role in the makeup of `PC1` than that of `PC2`,
* `ADHD.Tot` has a lot of weight for `PC1` and 
* Q15:Q18 carry a lot of positive weight while Q2:Q4 carry negative weight for `PC2` 

Next, we move on to the bi-plot for PC2 and PC3:

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC2 and PC3
biplot(pca_adhd, choices = c(2,3), scale = 0)
```

From the `PC2`-`PC3` biplot we can extend that:

* the observations follow a relatively consistent spread with some observations playing a larger role in `PC2` vs. `PC3` (ie. 60) while others play a larger role in `PC3` vs. `PC2` (ie. 39),
* Q1 and Q15 carry a lot of positive weight for `PC2` and 
* Q5 carries a lot of negative weight while Q1 carries a lot of positive weight for `PC3`

Finally, we explore the bi-plot for PC1 and PC3:

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC1 and PC3
biplot(pca_adhd, choices = c(1,3), scale = 0)
```

The `PC1`-`PC3` bi-plot has a similar layout to our first:

* the observations appear to play a larger role in the makeup of `PC1` than that of `PC2`,
* `ADHD.Tot` has a lot of weight for `PC1` and 
* Q1, Q2, Q4 carry a lot of positive weight while Q5, Q13, Q14 carry a lot of negative weight for `PC2`.

These plots are useful, but as a possible improvement we'll explore varimax. 

### Varimax rotation

Varimax rotation is used to simplify the expression of a particular sub-space. Coordinates remain unchanged while the sum of squared loadings are maximized.

We apply the **varimax()** function to the rotation variable of our principal components and plot the correlation of corresponding loadings: 

```{r}
# apply varimax 
var_adhd <- varimax(pca_adhd$rotation)

#var_adhd$loadings #corrplot is more succinct, easier to interpret

corrplot(var_adhd$loadings[,1:3], is.corr = FALSE)
```

We elect to interpret the correlation plot (over the loadings output) due to its clarity and succinctness. From the above output, we can see that:

* `PC1` is strongly (negatively) correlated with `ADHD.Total`,
* `PC2` is strongly (negatively) correlated with `ADHD.Q16`, and 
* `PC3` is strongly (positively) correlated with `ADHD.Q5`

Varimax provided in 2 lines of code what it took multiple plots, iterations, and interpretations to come to when referring to biplots: the strongest correlate for each principal component.

## PCA: MD and general

In the Appendix, we've also provided code (with a more succinct write up) for our application of principal component analysis to the `md` (mood disorder) and `genl` (general) datasets.

................................................................................


# 3. Support Vector Modeling

Support Vector Modeling (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.SVM can be used for both regression and classification tasks however is widely used in classification objectives.

```{r}
# #adhd <-md_df %>% tibble::add_column(Has_ADHD = if_else(genl$ADHD.Total<=35, "TRUE", "FALSE"))#add column 
# head(MD_df)
# #adhd[adhd==""] <- NA
# # Regression example
# set.seed(123)
# indexes = caret::createDataPartition(MD_df$MD.Q1a, p = .9, list = F)
# train = MD_df[indexes, ]
# test = MD_df[-indexes, ]
```

```{r warning=FALSE, message=FALSE, comment=FALSE}
# Fitting SVM to the Training set
# library(e1071)
# 
# classifier = svm(formula = MD.Q1a ~ .,
# 				data = train,
# 				type = 'C-classification',
# 				kernel = 'linear')

```


```{r}
# Predicting the Test set results
# y_pred = predict(classifier, newdata = test)
# y_pred
```

```{r}
# Making the Confusion Matrix
# cm = table(test, y_pred)
# 
# ```
# ```{r}
# # installing library ElemStatLearn
# library(ElemStatLearn)
#   
# # Plotting the training data set results
```



................................................................................

# References

In completing this assignment, reference was made to the following:

* MIT OpenCourseWare. (2018). **An Introduction to Clustering - Video 7: Hierarchical Clustering in R** [video]. Retrieved from: https://www.youtube.com/watch?v=GPOUGpF-Sno [hierarchical clustering]
* Bio Turing. (2018). **How to read PCA biplots and scree plots** [article]. Retrieved from: https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/

................................................................................


# Appendix

## Mood Disorder PCA

There are columns 15 columns for MD. we will follow the same process that we used for ADHD. 

```{r}
MD_df <- data %>% select(24:38)
head(MD_df)
# check for NAs
colSums(is.na(MD_df))
```

Above summary we see no NAs available in the `MD_df` dataset.

In MD.Q3 column scale is different than other columns, so to avoid scale factor set scale = True.

```{r}
# Compute principal component analysis.
pca_md <- prcomp(MD_df, scale = TRUE)
summary(pca_md)
```

From pca summary, PC1 shows 37% variance in the data, PC2 shows 50% variance in the data, PC3 shows 56% variance in the data and so on.

Plot the scree plot to disply the same, scree plot y-axis is variance nothing but the square of the SD. 
```{r}
# scree plot
plot(pca_md, type = 'l')
```

From scree plot PC1, PC2, PC3, and PC4 are enough to describe the data. 

From 'https://www.originlab.com/doc/Origin-Help/PrincipleComp-Analysis', found if the relationship is weak between variables, PCA does not work well to reduce data. For MD data more than 3 PC components describe the data, not sure how well PCA will work. 

### Correlation between variables and principal componenets:

Create another dataset use `cbind` with original ADHD data along with PC1, PC2, and PC3.

```{r}
# combine dataset
md_df_2 <- cbind(MD_df, pca_md$x)
# %>% column_spec(which(ADHD_df_2[,19] > 0.69) , color = 'white', background = 'maroon')
# cor plot
cor(MD_df, md_df_2[, 16:19]) %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down") 
```

Above cor table we can see 

* MD.Q1a, MD.Q1g, MD.Q1L,  and MD.Q2 have high negative  correlation with PC1. 
* MD.Q1g, and MD.Q1c  have positive correlation with PC2 and negative correlation with MD.Q3. 
* MD.Q1k has positive correlation with PC3 and MD.Q1c has negative correlation with PC3.
* MD.Q1m and MD.Q1L have negative correlation with PC4.

##### Bi-plot and  principal components

In Bi-plot red arrows represent the eigenvectors of each variable. This plot helps us to interpret the relationship between principal components.

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC1 and PC2
biplot(pca_md, choices = c(1,2), scale = 0)
```

From (PC1 and PC2 biplot) observations have low MD.Q3, MD.Q1h

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC2 and PC3
biplot(pca_md, choices = c(2,3), scale = 0)
```

(PC2 and PC3 biplot) 

* MD.Q1m negatively correlate with MD.Q1d. 
* MD.Q3 negatively correlate with MD.Q1k. 
* MD.Q1c negative correlate with MD.Q1L.

```{r, fig.height=10, fig.width=10}
# Bi-plot of PC1 and PC3
biplot(pca_md, choices = c(3,4), scale = 0)
```

From (PC1 and PC3 biplot)

* MD.Q1L negative correlate with MD.Q1i.
* MD.Q1b negative correlate with MD.Q1m.

We need better approach to examine the relationship. 

##### Using VARIMAX

```{r}
# apply varimax 
var_md <- varimax(pca_md$rotation)
var_md$loadings
corrplot(var_md$loadings[,1:4], is.corr = FALSE)
```

Varimax loading interprets : PC1 negatively co-relate with MD.Q1g , PC2 positively co-relate with MD.Q1i and PC3 positively co-relate with MD.Q1k and PC4 negatively co-relate with MD.Q1m.

## General PCA

PCA performs using entire dataset, exclude indivislau ADHD and MD column and take Total.ADHD and MD in to consideration.

Here we are using built-in R function *prccomp()* to perform the *Principal Component Analysis*. 

PCA can be applied only on numerical data. We have few Categorical variables in this dataset that need to be converted to numberical category. 

```{r}
# Check the data class
sapply(genl, class)
```
As we can see there are only 4 numeric data types and rest are factors. We need to convert all the factors into numeric.

```{r}
cols.num <- c("Sex","Race","Court.order","Education","Hx.of.Violence", "Disorderly.Conduct", "Suicide","Abuse","Non.subst.Dx","Subst.Dx")
genl[cols.num] <- sapply(genl[cols.num],as.numeric)
```


```{r}
# Check to see if all our variables in numeric.
sapply(genl, class)
```

```{r}
# Compute principal component analysis.
result.pca <- prcomp(genl, scale = TRUE)
```

Show the percentage of variances explained by each principal component.

```{r}
# Visualize eigenvalues (scree plot). 
fviz_eig(result.pca)
```
Individuals with a similar profile are grouped together.

```{r}
# Graph of individuals
fviz_pca_ind(result.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.

```{r}
# Graph of variables
fviz_pca_var(result.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
```{r}
# Biplot of individuals and variables
fviz_pca_biplot(result.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```

### Access to the PCA results

```{r}
# Eigenvalues
# eig.val <- get_eigenvalue(result.pca)
# eig.val
#   
# # Results for Variables
# res.var <- get_pca_var(result.pca)
# res.var$coord          # Coordinates
# res.var$contrib        # Contributions to the PCs
# res.var$cos2           # Quality of representation 
# # Results for individuals
# res.ind <- get_pca_ind(result.pca)
# res.ind$coord          # Coordinates
# res.ind$contrib        # Contributions to the PCs
# res.ind$cos2           # Quality of representation 
```
