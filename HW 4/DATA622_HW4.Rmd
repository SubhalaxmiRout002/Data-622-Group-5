---
title: "DATA622_HW4"
author: "Group 5"
date: "`r Sys.Date()`" # Due 5/7/2021
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

## Authorship

**Group 5:** 

* Don (Geeth) Padmaperuma,
* Subhalaxmi Rout, 
* Isabel R., and
* Magnus Skonberg

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r library, include= FALSE, comment=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(palmerpenguins)
library(e1071) 
library(caTools)
library(ggplot2)
library(GGally)
library(ggplot2) 
library(MASS) 
library(mvtnorm)
library(class)
library(dplyr)
library(mice)
library(corrplot)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(gbm)
library(plyr)
library(gridExtra)
library(randomForest)
library(kableExtra)
library(gbm)
library(xgboost)      # a faster implementation of gbm
library(Matrix)
library(factoextra)
library(cluster)
```

# Background

The purpose of this assignment was to explore clustering via k-means clustering or hierachical clustering, Principal Component Analysis, and Support Vector Modeling.


## Our Approach
[Insert text]

................................................................................


#  Data

The dataset is an ADHD mental health datset from a real-life research project.

We load in the data, pre-process it, verify the observations, and utilize the built-in glimpse() function to gain insight into the dimensions, variable characteristics, and value range:

```{r}
#Load in data
data <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/HW%204/ADHD_data.csv", stringsAsFactors = TRUE)

#data <-read.csv("ADHD_data.csv")
data2 <- data[,c(1,2,3,4,23,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54)]#remove individual ADHD self-report scale and Mood disorder questions.
adhd <-data2 %>% add_column(Has_ADHD = if_else(data$ADHD.Total<=35, "TRUE", "FALSE"))#add column 
head(adhd)
adhd[adhd==""] <- NA #replace empty strings with NAs
```



Being that we haven't worked with the ADHD dataset before, our exploratory data analysis (EDA) will be more in-depth. The depth will allow for greater understanding of the data at hand prior to applying clustering, Principal Component Analysis, and Support Vector Modeling.

```{r}
#Light EDA
glimpse(adhd)
summary(adhd)
```


We're dealing with a 175 observation x 54 variable dataframe with:

***
[I listed all the variables below but i'm not sure if I broke it out correctly Quantitative vs. Categorical. I'm also not sure which variables we should keep. I don't think Non-substance and Substance-related have much significance]


* `Sex`,
* `Race`,
* `ADHD Total`,
* `Mood Disorder Total`,
* `Individual Substances`(Alcohol, THC, Cocaine, Stimulants, Sedative-hypnotics, Opiods)<-Do we want to combine this?
* `Court Order`
* `Education`
* `History of Violence`
* `Disorderly Conduct`
* `Suicide Attempt`
* `Abuse History`
* `Non-substance-related`
* `Substance-related`
* `Psychiatric Medication`

***

* `ADHD Total`, a categorical, character-based variable, as our dependent variable,
* `Mood Disorder Total`,`Individual Substances`(Alcohol, THC, Cocaine, Stimulants, Sedative-hypnotics, Opiods), `Abuse History`, `Non-substance related`, `Substance-related`,and `Psychiatric Meds`, all quantitative variables of type dbl or int, as independent variables, and
* `Sex`, `Race`, `Court Order`, `Education`, `History of Violence`, `Disorderly Conduct`, and `Suicide Attempt`,  all categorical, character-based variables, as independent variables.


Of the above variables, we can see that `[Variable]` does not appear to provide much insight. We'll drop this variable, explore a clearer visualization of NA counts and then deal with our NA values: 


# 1. Clustering

Clustering attempts to find clusters of observations within a dataset and finds structure within a dataset rather than predicting the value of some response variable.

```{r}
fviz_nbclust(adhd, kmeans, method = "wss")
```

```{r}
#calculate gap statistic based on number of clusters
gap_stat <- clusGap(adhd,
                    FUN = kmeans,
                    nstart = 25,
                    K.max = 10,
                    B = 50)

#plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)
```

```{r}
#make this example reproducible
set.seed(1)

#perform k-means clustering with k = 4 clusters
km <- kmeans(adhd, centers = 4, nstart = 25)

#view results
km
```

................................................................................


# 2. Principal Component Analysis

Principal Component Analysis (PCA) is a way of reducing the dimensions of a given dataset by extracting new features from the original features present in the dataset. The “new” variables after PCA are all independent of one another.PCA wont reduce the number of features / variables in your data.

Here we are using built-in R function *prccomp()* to perform the *Principal Component Analysis*. 

PCA can be applied only on numerical data. We have few Categorical variables in this dataset that need to be converted to numberical category. 

```{r}
# Check the data class
sapply(genl, class)
```
As we can see there are only 4 numeric data types and rest are factors. We need to convert all the factors into numeric.

```{r}
cols.num <- c("Sex","Race","Court.order","Education","Hx.of.Violence", "Disorderly.Conduct", "Suicide","Abuse","Non.subst.Dx","Subst.Dx")
genl[cols.num] <- sapply(genl[cols.num],as.numeric)
```


```{r}
# Check to see if all our variables in numeric.
sapply(genl, class)
```

```{r}
# Compute principal component analysis.
result.pca <- prcomp(genl, scale = TRUE)
```

Show the percentage of variances explained by each principal component.

```{r}
# Visualize eigenvalues (scree plot). 
fviz_eig(result.pca)
```
Individuals with a similar profile are grouped together.

```{r}
# Graph of individuals
fviz_pca_ind(result.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.

```{r}
# Graph of variables
fviz_pca_var(result.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
```{r}
# Biplot of individuals and variables
fviz_pca_biplot(result.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```

### Access to the PCA results
```{r}
# Eigenvalues
eig.val <- get_eigenvalue(result.pca)
eig.val
  
# Results for Variables
res.var <- get_pca_var(result.pca)
res.var$coord          # Coordinates
res.var$contrib        # Contributions to the PCs
res.var$cos2           # Quality of representation 
# Results for individuals
res.ind <- get_pca_ind(result.pca)
res.ind$coord          # Coordinates
res.ind$contrib        # Contributions to the PCs
res.ind$cos2           # Quality of representation 
```

```{r}
#calculate principal components
results <- prcomp(adhd, scale = TRUE)

#reverse the signs
results$rotation <- -1*results$rotation

#display principal components
results$
  
#reverse the signs of the scores
results$x <- -1*results$x

```

```{r}
biplot(results, scale = 0)
```

```{r}
#calculate total variance explained by each principal component
results$sdev^2 / sum(results$sdev^2)

```

```{r}
#calculate total variance explained by each principal component
var_explained = results$sdev^2 / sum(results$sdev^2)

#create scree plot
qplot(c(1:4), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```

................................................................................

# 3. Support Vector Modeling

Support Vector Modeling (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.SVM can be used for both regression and classification tasks however is widely used in classification objectives.

```{r}
library(e1071)
library(caret)
 
# Regression example
set.seed(123)
indexes = createDataPartition(adhd$Has_ADHD, p = .9, list = F)
train = adhd[indexes, ]
test = adhd[-indexes, ]
 
model_reg = svm(Has_ADHD~., data=train)
print(model_reg)
 
pred = predict(model_reg, test)
 
x=1:length(test$Has_ADHD)
plot(x, test$Has_ADHD, pch=18, col="red")
lines(x, pred, lwd="1", col="blue")
 
# accuracy check 
mse = MSE(test$medv, pred)
mae = MAE(test$medv, pred)
rmse = RMSE(test$medv, pred)
r2 = R2(test$medv, pred, form = "traditional")
 
cat(" MAE:", mae, "\n", "MSE:", mse, "\n", 
    "RMSE:", rmse, "\n", "R-squared:", r2)
 
```



```{r}
# Encoding the target feature as factor
adhd$Has_ADHD = factor(adhd$ADHD.Total, levels = c(0, 1))
```

```{r}
# Splitting the dataset into the Training set and Test set
set.seed(123)
split = sample.split(adhd$Has_ADHD, SplitRatio = 0.75)

training_set = subset(adhd, split == TRUE)
test_set = subset(adhd, split == FALSE)

```


```{r}
# Fitting SVM to the Training set
classifier = svm(formula = ADHD.Total ~ .,
				data = training_set,
				type = 'C-classification',
				kernel = 'linear')
```

```{r}
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set])

```

................................................................................


# References

In completing this assignment, reference was made to the following:

*Statology. (2020). **K-Means Clustering in R: Step-by-Step Example** [article]. Retrieved from:
https://www.statology.org/k-means-clustering-in-r/
* Towards Data Science. (2019). **Principal Component Analysis (PCA) 101, using R** [article]. Retrieved from: https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff
*Medium. (2019). **Principal Component Analysis(PCA) in Machine Learning Made Easy** [article].
Retried from: https://shiva1gandluri.medium.com/principal-component-analysis-pca-in-machine-learning-c3f239249b73
* monkeyLearn. (2017). **An Introduction to Support Vector Machines (SVM)**[article]. Retrieved from:
https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/
*Statology. (2020.) **Principal Components Analysis in R: Step-by-Step Example** [article]. Retrieved from:
https://www.statology.org/principal-components-analysis-in-r/
* Towards Date Science. (2018). **Support Vector Machine — Introduction to Machine Learning Algorithms** [article]. Retrieved from:
https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47
*
