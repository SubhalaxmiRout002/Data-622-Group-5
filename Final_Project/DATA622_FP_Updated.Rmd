---
title: "DATA622_FP"
author: "Group 5"
date: "`r Sys.Date()`" # Due 5/21/2021
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: paper
    highlight: monochrome
---

## Authorship

**Group 5:** 

* Don (Geeth) Padmaperuma,
* Subhalaxmi Rout, 
* Isabel Ramesar, and
* Magnus Skonberg

```{r setup, include=FALSE}
#Standardize chunk-knitting
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(123) #ensure reproducibility
```

```{r library, include= FALSE}
#Load relevant libraries
library(tidyverse)
library(caret)
library(ggplot2) 
library(MASS) 
library(dplyr)
library(kableExtra)
library(plyr) #revalue 'Dependent'
library(mice) #pmm imputation
library(corrplot) #correlation matrix
library(Boruta) #Utilize Boruta for feature ranking and selection
library(gridExtra) #output plots via grid.arrange
library(car) #outlier handling
library(caTools)
library(keras) #NN approach 2
library(neuralnet) #NN approach 3
require(reshape2) # small multiple density plot
library(class) # kNN 
library(rpart) # DT
library(rpart.plot)# DT
#library(tensorflow) 
#install_tensorflow()
#Utilize customized functions
plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}
```

# Background

The purpose of our Final Project was to explore the application of Neural Networks to loan approval data to then back compare model performance with a variety of Classification algorithms (ie. KNN, DT, RF, GBM).

## Neural Networks

Neural networks form the basis of Deep Learning, an extension of Machine Learning, where algorithms are inspired by the structure of the human brain. They take in data, train themselves to recognize patterns therein, and then predict outputs for similar, unseen data.

```{r image='asis', out.width='600px'}
download.file(
    url='https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png',
    destfile='image1.jpg',
    mode='wb')
knitr::include_graphics(path='image1.jpg')
```

Neural networks are made up of layers of nodes. They contain an input layer, one or more hidden layers, and an output layer. Nodes are interconnected with associated weights and thresholds. When a node is above its specified threshold, the node is activated and data is sent to the next layer of the network. Otherwise, data is not fed forward.

The power of a neural network lies in its ability to fine-tune upon countless iterations. *Back-propagation* allows for continuous model accuracy improvement. Weights are adjusted based on the magnitude of error at the output layer, and continuous refinement allows for predictive accuracy improvements.

## Our Approach

We'll start by (re) exploring and preparing the loan dataset, progress to building our neural network model, and then compare and contrast `loan approval status` prediction accuracy for our neural network model vs. decision tree, random forest, and gradient boosting models.

***


# Loan Approval Data

A loan is when money is transferred from one party to another under the prospect that the lender (loan giver) will be repaid in full *with interest* by the lendee (loan receiver). 

The profit, for the lendor, comes from the interest they are paid by the lendee and thus, as a core part of their business model, it’s important for banks and credit companies alike to be able to depend upon the fact that their loan (and interest) will be repaid in full.

With this motivation in mind, we (re) explore and prepare our loan approval dataset in order to construct a more precise neural network model (later). Being that we've explored this data before, we build upon the core takeaways of our past exploration while simultaneously pushing the bounds of our understanding to a deeper level. *Rudimentary (early) EDA steps will be summarized and/or excluded from the write up and included in the Appendix in favor of output that provides greater context and insight.*

Prior to commencing EDA, we revisit the corresponding data directory:

* `LoanID`: unique loan ID
* `Gender`: applicant gender (Male/Female)
* `Married`: applicant marriage status (Yes/No)
* `Dependents`: number of dependents for applicant (0, 1, 2, 3+)
* `Education`: applicant college education status (Graduate / Not Graduate)
* `Self_Employed`: applicant self-employment status (Yes/No)
* `ApplicantIncome`: applicant income level
* `CoapplicantIncome`: co-applicant income level (if applicable)
* `LoanAmount`: loan amount requested (in thousands)
* `Loan_Amount_Term`: loan term (in months)
* `Credit_History`: credit history meets guidelines (1/0)
* `PropertyArea`: property location (Urban/Semi Urban/Rural)
* `Loan_Status`: loan approved (Yes/No). **target variable**

***


# Data Exploration & Preparation

To start, we load in our data, replace empty strings with NAs, observe the first 6 observations of our dataset to refamiliarize ourselves with the format of our data and then use R's built-in glimpse() and summary() functions to revisit data types and value ranges.

```{r, include=F}
#Load in data
loan <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/Final_Project/Loan_approval.csv", stringsAsFactors = TRUE)
loan[loan==""] <- NA #replace empty strings with NAs
#head(loan) #verify 1st 6 observations
#Light EDA
glimpse(loan)
summary(loan)
```

We're dealing with a 614 observation x 13 variable dataframe with `Loan_Status` as our dependent, categoric variable, `ApplicantIncome`, `CoApplicantIncome`,`LoanAmount`, `Loan_Amount_Term`, and `Credit_History` as independent, numeric variables, and `Loan_ID`, `LoanGender`, `Married`, `Dependents`, `Education`, `Self_Employed`, `Property_Area`, and `Loan_Status` as independent, categoric variables.

From above, we extend that `Loan_ID` can be dropped, `ApplicantIncome` and `CoApplicantIncome` can be combined to create a `TotalIncome` variable, and observations with a "3+" label in `Dependents` should be re-labelled as "3" so that data follows a consistent format and imputation can be performed as a next step.

## NA Values

We pre-process our data (as described above), visualize and handle NA values:

```{r}
#Pre-process dataset for easier interpretation
loan <- subset(loan, select = -c(1) ) #drop Loan_ID from consideration
loan$TotalIncome <- loan$CoapplicantIncome + loan$ApplicantIncome #create TotalIncome variable
loan <- subset(loan, select = -c(6,7) ) #drop CoapplicantIncome and ApplicantIncome
loan$Dependents <- revalue(loan$Dependents, c("3+"="3")) #relabel Dependents "3+" value as "3"
#Visualize NA counts
colSums(is.na(loan)) 
```

```{r, include=F}
#Handle NAs: apply predictive mean matching to loan data
loan <- mice(loan, m = 1, method = "pmm", seed = 500)
loan <- mice::complete(loan, 1)
```

We re-assign the "3+" value of the `Dependents` variable to provide consistent leveling and enable **pmm** (predictive mean matching). Predictive mean matching calculates the predicted value for our target variable, and, for missing values, forms a small set of “candidate donors” from the complete cases that are closest to the predicted value for our missing entry. Donors are then randomly chosen from candidates and imputed where values were once missing. *To apply pmm we assume that the distribution is the same for missing cells as it is for observed data, and thus, the approach may be more limited when the % of missing values is higher.*

Once we've imputed missing values, we verify whether our operation was successful:

```{r}
#verify absence of NA values in the dataset
colSums(is.na(loan))
```

Imputation was a success and data pre-processing has been completed. From this point we proceed to the observance of feature correlation.

## Correlation and Variable Importance

To identify feature correlation - how strongly independent variables are related to one another and how strongly these variables relate to our dependent variable (`Loan_Status`), we consider a correlation matrix with a threshold of 0.3: 

```{r}
#Utilize custom-built correlation matrix generation function
plot_corr_matrix(loan, 0.3)
```

From the correlation matrix we can extend that:

* `Credit_History` is our strongest predictor / strongly correlated with `Loan_Status`, and
* `Gender` and `Married`, `Married` and `Dependents`, `LoanAmount` and `TotalIncome` appear to be correlated with one another and indicative that multicollinearity may be a concern for our data.

We varied the threshold value for our correlation matrix and found that, aside from `Credit_History`, our other independent variables were relatively poor predictors of `Loan_Status`, making it worth exploring variable importance:

```{r, comment=FALSE, warning=FALSE, message=FALSE, fig.height = 8, fig.width = 10}
#NOTE: COMMENTED OUT BELOW DUE TO LONG COMPILATION TIME. UNCOMMENT BEFORE FINAL SUBMISSION.
# Perform Boruta search
#boruta_output <- Boruta(Loan_Status ~ ., data=na.omit(loan), doTrace=0, maxRuns = 1000)
#Get significant variables including tentatives
#boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)
# Plot variable importance
#plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")
```

Our utilization of the **Boruta** function for feature ranking and selection indicate that:

* `Credit_History`, `TotalIncome`, `LoanAmount`, and `Self_Employed` are strong predictors,
* `Property_Area` is a moderate predictor, and
* `Married`, `Loan_Amount_Term`, `Education`, `Gender`, and `Dependents` are all poor predictors.

With feature importance in mind, we drop the poor predictors from consideration. Dropping these variables also addresses concerns of applicant discrimination (ie. rejection based on `Gender`) and thus we address two concerns with this act of feature selection.

```{r, include=F}
library(data.table)
#Subset data based on predictor strength
loan <- subset(loan, select = -c(1, 2, 3, 4, 7) ) #drop poor predictors
#head(loan) #verify
# created 5 datasets for LR, Neural nets, NN using keras, kNN, Decison Tree
loan_new2 <- loan
loan_nn <- loan
loan_keras <- loan
loan_knn <- loan
loan_dt <-  loan
```

## Independent Variable Distributions

With our loan dataset properly subset, we proceed to observing the distributions of our independent variables. First we observe numeric distributions:

```{r}
#convert CreditHistory to type factor
loan$Credit_History <- factor(loan$Credit_History)
#levels(loan$Credit_History) #verify
#Numeric distributions
loan %>%
    keep(is.numeric) %>%
    gather() %>% 
    ggplot(aes(value)) +
        facet_wrap(~ key, scales = "free", ncol=1) +
        geom_histogram(bins=90,color="darkblue", fill="lightblue")
```

From the above figures we observe that `LoanAmount` and `TotalIncome` appear to be right skewed normal and there are a number of noteworthy outliers for both distributions. From this, we note the importance of outlier-handling and scaling as critical steps in building our neural network model.

Next, we explore our categorical variables:

```{r}
#Categoric distributions
##Self_Employed
p1 <- loan %>% dplyr::select(1,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Self_Employed, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")
##Self_Employed
p2 <- loan %>% dplyr::select(3,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Credit_History, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")
##Property_Area
p3 <- loan %>% dplyr::select(4,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Property_Area, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")
grid.arrange(p1, p2, p3, nrow = 2, ncol = 2)
```

From the above figures we can extend:

* non self employed outnumbers self employed on a 5:1 basis, 
* credit history meeting qualifications outnumbers not meeting qualifications on a 5:1 basis,
* properties in semiurban areas make up a slight majority, and
* with regard to loan approval, it appears that being self-employed, having a strong credit history, and living in a semiurban area are advantageous. *The strongest categorical predictor appears to be that the applicant have a credit history that meets qualifications.*

With a relatively thorough exploratory analysis under our belt, we move on to building our neural network model.

***


# Model Building

We'll utilize the *holdout-validation method* for evaluating model performance. We train-test split our data using a 75:25 partition, build our model on the training set and then evaluate its performance on the test set.

## Neural Network #1 (nnet)

To start, we compute our "barrier value" and then partition our data based on this value, with 75% of our data going in the training set and 25% of our data going in the test set.

```{r}
set.seed(123) #replicability
bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value
#Subset: train-test split based on partition value
train <- loan[partition, ] 
test <- loan[-partition, ]
#print(dim(train)) #460 x 6
#print(dim(test)) #154 x 6
```

We set our training algorithm’s parameters and then train our model using the train() function with "nnet" passed in as the method and "scale" and "center" passed in so that numeric variables are standardized.

```{r, include=F}
set.seed(123) #replicability
#Specify training algorithm parameters
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5)
#Train neural net model and standardize variables via preProcess method
nnet_model1 <- train(train[,-5], train$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)
```

With our "baseline model" trained, we proceed to model evaluation. We verify the baseline accuracy (0.676) and then evaluate our model's performance against this "baseline". We generate predictions based on the training set and then output these predictions as a confusion matrix and then we do the same with our test data.

```{r}
set.seed(123) #replicability
#round(prop.table(table(train$Loan_Status)),3)   #Baseline accuracy Y: 0.685
#Training predictions
nnPred_train <-predict(nnet_model1, train)
#Training confusion matrix
table(train$Loan_Status, nnPred_train)
round((310+78)/nrow(train),3)                    
#Test predictions
nnPred_test <-predict(nnet_model1, test)
#Test confusion matrix
table(test$Loan_Status, nnPred_test)
round((106+21)/nrow(test),3) 
```

From above, we observe a training accuracy of 84.3% and a test accuracy of 82.5% which is an improvement of more than 15% over our "baseline accuracy".

By merely applying a neural network model to our dataset, we see major improvements in predictive capability. Next, we see if we can take the model further. If we can improve model performance by handling outliers and creating features prior to feeding the model.

### Optimization Attempt

We explore the affects of outlier handling and feature creation on model performance to determine if either step improves our model.

We start by re-visiting the distribution of outliers via boxplot:

```{r, fig.height=8, fig.width=8} 
set.seed(123) #replicability
#Observe the affect of outlier-handling on model performance (if any)
#Confirm the presence of influential observations
p4 <- ggplot(loan) +
  aes(x = Loan_Status, y = LoanAmount) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
p5 <- ggplot(loan) +
  aes(x = Loan_Status, y = TotalIncome) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
grid.arrange(p4, p5, nrow = 1, ncol = 2)
```

From above we can see that outliers appear to be a concern for our model. 

**Outlier Handling**

To rectify the situation, we identify the outliers using the boxplot.stats() function, filter for corresponding observations, remove outliers from our dataset, and revisit model performance.

```{r, include=F, eval=F}
 set.seed(123) #replicability
#Identify outlier locations for LoanAmount, TotalIncome
out1 <- boxplot.stats(loan$LoanAmount)$out
outliers1 <- which(loan$LoanAmount %in% c(out1))
out2 <- boxplot.stats(loan$TotalIncome)$out
outliers2 <- which(loan$TotalIncome %in% c(out2))
outliers <- c(outliers1, outliers2) #merge lists
outliers <- unique(outliers) #remove repeat values
#Remove outliers
loan <- loan[-outliers,]
#Observe affect on model performance
bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition2 <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value
#Subset: train-test split based on partition value
train2 <- loan[partition2, ] 
test2 <- loan[-partition2, ]
#Train neural net model and standardize variables via preProcess method
nnet_model2 <- train(train2[,-5], train2$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)
#Training predictions
nnPred_train2 <-predict(nnet_model2, train2)
#Training confusion matrix
table(train2$Loan_Status, nnPred_train2)
round((282+63)/nrow(train2),3) #0.839               
#Test predictions
nnPred_test2 <-predict(nnet_model2, test2)
#Test confusion matrix
table(test2$Loan_Status, nnPred_test2)
round((94+17)/nrow(test),3) #0.721
```

Outlier-handling led to a slight performance reduction for training and significant performance reduction for test data. As such we elected not to include outlier-handling as an optimization step. *Note: corresponding code has been included in the Appendix.*

We proceeded to observe the affect of feature creation on model performance. 

**Feature Creation**

We wanted to see if adding certain combinations of features would improve our predictive accuracy. We tested the inclusion of variables for:

* self employed with high income,
* semiurban property with qualified credit history,
* not self employed with low loan amount, and
* low income with high loan amount.

The inclusion of these variables, and feature creation in general, *slightly* reduced the performance of our model and so we elected to exclude it as a modeling optimization step. *Note: corresponding code has been included in the Appendix.*

```{r, include=F, eval=F}
set.seed(123) #replicability
#Observe the affect of feature creation on model performance (if any)
#if self employed and income greater than
loan$hiINC_SE <- as.factor(ifelse(loan$TotalIncome >= 7522 & loan$Self_Employed == "Yes", 1, 0))
#if semiurban property and credit history 1
loan$SEMI_CH <- as.factor(ifelse(loan$Property_Area == "Semiurban" & loan$Credit_History == 1, 1, 0))
#if not self employed and loan amount below
loan$notSE_CH <- as.factor(ifelse(loan$Self_Employed == "No" & loan$LoanAmount <= 100.0, 1, 0))
#if income below and loan amount above
loan$loINC_hiAMT <- as.factor(ifelse(loan$TotalIncome <= 4166 & loan$LoanAmount >= 166.8, 1, 0))
#head(loan) #verify
```

```{r, include=F, eval=F}
set.seed(123) #replicability
bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition2 <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value
#Subset: train-test split based on partition value
train2 <- loan[partition2, ] 
test2 <- loan[-partition2, ]
#Train neural net model and standardize variables via preProcess method
nnet_model2 <- train(train2[,-5], train2$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)
#Training predictions
nnPred_train2 <-predict(nnet_model2, train2)
#Training confusion matrix
table(train2$Loan_Status, nnPred_train2)
round((282+63)/nrow(train2),3) #0.839 - LOWER                  
#Test predictions
nnPred_test2 <-predict(nnet_model2, test2)
#Test confusion matrix
table(test2$Loan_Status, nnPred_test2)
round((94+17)/nrow(test),3) #0.721 - LOWER
```

Being that each of our optimization attempts were fruitless, we next opted to explore two alternative approaches to neural networks to compare the predictive accuracy between approaches.

## Neural Network #2 (keras)

For our first "alternative" neural network approach, we consider the `keras` package and explore the characteristics of the model and means of optimizing the model:

* layers which are combined into a network (or model),
* input data and corresponding targets,
* the loss function which defines the feedback signal used for learning, and
* the optimizer which determines how learning proceeds.

We pre-process our data (converting variables to the proper type and changing categoric variable values from a character to a numeric-base) and then visualize the layers of our neural network:

```{r}
set.seed(123) #replicability
#Preprocess data
loan_keras <-  loan_keras[, c(1, 3, 4, 2, 6, 5)]
# Convert Credit_History numeric type
loan_keras$Credit_History <- as.numeric(loan_keras$Credit_History)
# Change Variables values
loan_keras$Self_Employed <- ifelse(loan_keras$Self_Employed == "Yes", 1, 0)
loan_keras$Loan_Status <- ifelse(loan_keras$Loan_Status=="Y", 1, 0)
loan_keras$Property_Area <- case_when(
  loan_keras$Property_Area == "Semiurban" ~ 2,
  loan_keras$Property_Area == "Urban" ~ 1,
  loan_keras$Property_Area == "Rural" ~ 0,
)
#loan_keras <- loan_keras %>% mutate_if(is.factor, as.numeric) # convert factor to numeric
#Build initial keras NN model
n <- neuralnet(Loan_Status ~ 
                 Self_Employed + 
                 Credit_History + 
                 Property_Area + 
                 LoanAmount + 
                 TotalIncome,
               data = loan_keras,
               hidden = c(4,2),
               linear.output = F,
               lifesign = 'full',
               rep=1)
#Visualize our initial keras NN
plot(n,
     col.hidden = 'darkgreen',
     col.hidden.synapse = 'darkgreen',
     show.weights = F,
     information = F,
     fill = 'lightblue', rep = "best")
```

From the above visualization, we observe 5 input nodes, 2 hidden layers (1 with 4 neurons and 1 with 2 neurons), and 1 output node. As is the case with neural networks, all nodes are interconnected.

As a next step, we convert our data into matrix form, train-test split (using an 80/20 partition), and normalize (center and scale). We then build, compile, and train our model using the keras_model_sequential(), compile(), and fit() functions respectively.

```{r, echo=F, eval=T, include=F}
set.seed(123) #replicability
# Matrix
loan_keras <- as.matrix(loan_keras)
dimnames(loan_keras) <- NULL
# Partition
ind <- sample(2, nrow(loan_keras), replace = T, prob = c(.8, .2))
training_keras <- loan_keras[ind==1,1:5]
testing_keras <- loan_keras[ind==2, 1:5]
trainingtarget <- loan_keras[ind==1, 6]
testingtarget <- loan_keras[ind==2, 6]
# Normalize
m <- colMeans(training_keras)
s <- apply(training_keras, 2, sd)
training_keras <- scale(training_keras, center = m, scale = s)
testing_keras <- scale(testing_keras, center = m, scale = s)
```

We use the layer_dense() function to create the hidden layers for our network with "relu" (rectified linear unit) as our activation function, 5 inputs, 4 nodes in our 1st hidden layer, 2 nodes in our 2nd hidden layer, and 1 output node (as in our earlier visualization).

We compile our model using the following 3 arguments:

* **loss**: we elect "mse" (mean square error) which takes the sum of squared distances between target and prediction
* **optimizer**:  default is rmsprop(lr stands for learning rate)
* **metrics**: list of metrics to be evaluated by the model. We elected accuracy.

Finally, we fit our model (with a validation proportion of 0.1 and 100 epochs) and visualize the model's accuracy and loss for training and validation sets:


```{r, message=F}
library(keras)
set.seed(123) #replicability
#Build model
model_keras <- keras_model_sequential()
model_keras %>% 
         layer_dense(units = 4, activation = 'relu', input_shape = c(5)) %>%
         layer_dense(units = 2, activation = 'relu') %>%
         layer_dense(units = 1)
#Compile model
model_keras %>% compile(loss = 'mse',
                  optimizer = optimizer_rmsprop(lr= 0.005),
                  metrics = c("accuracy"))
#Train model
model_keras_train <- model_keras %>%
         fit(training_keras,
             trainingtarget,
             epochs = 100,
             validation_split = 0.1
             )
#Visualize
plot(model_keras_train)
```

The first graph is for loss whereas the second is for accuracy. There is little difference in loss for training vs. validation sets whereas when we consider accuracy, we see that training and validation accuracy start with a low accuracy (which makes sense) and then diverge shortly before the 10th epoch with validation settling ~0.85 and training settling ~0.8. 

This appears to indicate that over-training is not a concern but we'll explore further via evaluation metrics:

```{r}
set.seed(123) #replicability
#Evaluate model on training data
model_keras %>% evaluate(training_keras, trainingtarget)
pred_keras_train <- model_keras %>% predict(training_keras)
#Evaluate model on test data
model_keras %>% evaluate(testing_keras, testingtarget)
pred_keras_test <- model_keras %>% predict(testing_keras)
```

Our evaluation confirms that the keras model performs better on (unseen) test data.

We explore accuracy metrics for test data in confusion matrix form for further interpretation:

```{r}
set.seed(123) #replicability
# Confusion matrix and misclassification- test data
pred_keras_1 <- ifelse(pred_keras_test>0.5, 1, 0)
tab4 <- table(Predicted = pred_keras_1,
      Actual = testingtarget)
tab4
sum(diag(tab4))/sum(tab4)
```

With an accuracy of ~86%, our keras model classifies loan approvals with a high degree of accuracy while misclassifying countless loan rejections as approvals. There were 14 misclassified rejections vs. just 4 misclassified approvals.

As such, we'll attempt to optimize this model.

### Optimization Attempt

We revisit our model with varied neurons / layer_dropout. 

Increasing the number of neurons may improve model performance but that is not always the case and so we specify our "rate" as 0.4, re-specify our validation set as 0.2, and revisit our visualization and evaluation metrics to observe the effect:

```{r}
set.seed(123) #replicability
#Create model2
model_keras_2 <- keras_model_sequential()
model_keras_2 %>% 
         layer_dense(units = 100, activation = 'relu', input_shape = c(5)) %>%
         layer_dropout(rate = 0.4) %>%
         layer_dense(units = 1)
#Compile model2
model_keras_2 %>% compile(loss = 'mse',
                  optimizer = optimizer_rmsprop(lr= 0.005),
                  metrics = c("accuracy"))
#Fit model2
model_keras_train2 <- model_keras_2 %>%
         fit(training_keras,
             trainingtarget,
             epochs = 100,
             batch_size = 32,
             validation_split = 0.2
             )
#Visualize
plot(model_keras_train2)
```

Our loss visualization y axis covers a different range and it appears that our training loss is around ~0.15 while our validation loss stabilizes around ~0.17. When we consider that our training set was reduced in size this makes sense.

Our accuracy visualization's y axis also covers a different range and and we observe that training and validation start with low accuracies, follow each other with training accuracy slightly higher (0.825 vs. 0.82) and conclude with training accuracy and validation accuracy criss-crossing twice before epoch 100.

We explore further via evaluation metrics and confusion matrix:

```{r}
set.seed(123) #replicability
#Evaluate model on training data
model_keras_2 %>% evaluate(training_keras, trainingtarget)
pred_keras_train2 <- model_keras_2 %>% predict(training_keras) #loss: 0.1338, accuracy: 0.8390
#Evaluate model on test data
model_keras_2 %>% evaluate(testing_keras, testingtarget)
pred_keras_test2 <- model_keras_2 %>% predict(testing_keras) #loss: 0.1413, accuracy: 0.8632
# Confusion matrix and misclassification 
pred_keras_2 <- ifelse(pred_keras_test2>0.5, 1, 0)
tab5 <- table(Predicted = pred_keras_2,
      Actual = testingtarget)
tab5
sum(diag(tab5))/sum(tab5)
```

Our accuracy on (unseen) test data improved from 86% to greater than 87%. Additionally, although the model still performs better in predicting approvals, its rejection predictions improved from 14 misclassifications to 13 rejections.


## Neural Network #3 (neural net)

For our second "alternative" neural network approach, we consider the `neuralnet` package and explore the characteristics of the model and means of optimizing the model.

We start by pre-processing our data:

* converting `Credit_History` to type numeric,
* re-assigning `Self_Employed` as Yes = 1, No = 0,
* re-assigning `Property_Area` as Semi-urban = 2, Urban = 1, and Rural = 0,
* re-assigning `Loan_Status` as Yes = 1, No = 0, and
* normalizing our variables to bring all data to a 0-to-1 scale via the following equation:

$$
Transformed.Values = \frac{(Values - Min)}{(Max - Min)}
$$
With our data in proper form (consistent, normalized variables), we train-test split using an 80/20 partition, train and visualize our model:

```{r}
set.seed(123) #replicability
# Convert Credit_History numeric type
loan_nn$Credit_History <- as.numeric(loan_nn$Credit_History)
# Change Variables values
loan_nn$Self_Employed <- ifelse(loan_nn$Self_Employed == "Yes", 1, 0)
loan_nn$Loan_Status <- ifelse(loan_nn$Loan_Status=="Y", 1, 0)
loan_nn$Property_Area <- case_when(
  loan_nn$Property_Area == "Semiurban" ~ 2,
  loan_nn$Property_Area == "Urban" ~ 1,
  loan_nn$Property_Area == "Rural" ~ 0,
)
# Min-Max Normalization
loan_nn$Self_Employed <- (loan_nn$Self_Employed - min(loan_nn$Self_Employed))/(max(loan_nn$Self_Employed) - min(loan_nn$Self_Employed))
loan_nn$LoanAmount <- (loan_nn$LoanAmount - min(loan_nn$LoanAmount))/(max(loan_nn$LoanAmount) - min(loan_nn$LoanAmount))
loan_nn$Credit_History <- (loan_nn$Credit_History - min(loan_nn$Credit_History))/(max(loan_nn$Credit_History)-min(loan_nn$Credit_History))
loan_nn$Property_Area <- (loan_nn$Property_Area - min(loan_nn$Property_Area))/(max(loan_nn$Property_Area)-min(loan_nn$Property_Area))
loan_nn$TotalIncome <- (loan_nn$TotalIncome - min(loan_nn$TotalIncome))/(max(loan_nn$TotalIncome)-min(loan_nn$TotalIncome))
# Train-test split
ind <- sample(2, nrow(loan_nn), replace = TRUE, prob = c(0.8, 0.2))
training_nn <- loan_nn[ind==1,]
testing_nn <- loan_nn[ind==2,]
#Train model
n1 <- neuralnet(Loan_Status ~ Self_Employed + LoanAmount + Credit_History + Property_Area + TotalIncome,
               data = training_nn,
               hidden = 1,
               err.fct = "ce",
               linear.output = FALSE
               )
#Visualize model
plot(n1, rep = "best")
```

The plot of our neural network above shows 5 input nodes, 1 hidden layer with 1 node, and 1 output node. This is *by far* the simplest neural network we've visited thus far.

After this point, we verified the predictive accuracy of our model via node output calculations with sigmoid activation function. *Note: corresponding code has been included in the Appendix.*

```{r, include=F}
# values extracted for manual entry
head(training_nn[1,])
#manually calculate first value of output node and compare with training model output
in6 <- 10.47375 + (0*-5.36828) + (0.1432706*2.7186) + (1*-13.27065) + (0.5 * -1.77825) + (0.05539355 * 8.03783)
out6 <- 1/(1+exp(-in6))
in7 <- 1.71561 +(-4.21959*out6)
out7 <- 1/(1+exp(-in7))
#Output actual node 7 output vs. predicted output for our 1st entry
paste0("Node 7 Output : " ,out7)
output <- compute(n1, training_nn[,-5])
paste0("First predicted value of NN : ", head(output$net.result, n = 1))
```

We then proceed to attempt to optimize our neuralnet model via feature selection.

### Optimization Attempt

Feature selection is used to select the most important features in a set. It's a balancing act of selecting the fewest featuers that provide the greatest representative capability of the data and thus the strongest predictive accuracy.

We utilized forward feature selection, starting with no features and adding them back in one-by-one to find the strongest combination. During our exploratory data analysis, we found that `Credit_History` was the most important feature, so we began with this feature and tested combinations with other features until we arrived at: 

```{r}
set.seed(123)
#Adding other variables did not help model performance:
#Credit_History + Property_Area (0.871 test)
#Credit_History + TotalIncome (0.871 test)
#Credit_History + LoanAmount (0.871 test)
#Credit_History + SelfEmployed (0.871 test)
n2 <- neuralnet(Loan_Status ~ Credit_History, 
               data = training_nn,
               hidden = 1,
               err.fct = "ce",
               linear.output = FALSE
               )
plot(n2, rep = 'best')
```

An even simpler model than that we had visited before: 1 input node, 1 hidden layer with 1 node, and 1 output node. From this we can derive that our strongest factor, `Credit_History` provides enough signal to accurately predict `Loan_Status`.

We then evaluate our model's performance on training vs. test data and visit the corresponding confusion matrix:

```{r}
# Confusion Matrix & Misclassification Error - training data
output <- compute(n2, training_nn[,-5])
p1 <- output$net.result
pred1 <- ifelse(p1>0.5, 1, 0)
tab1 <- table(Prediction =  pred1, Actuals = training_nn$Loan_Status)
#tab1
paste0("Misclassification Error of training data: ", round(100 - sum(diag(tab1))/sum(tab1)*100,2))
paste0("Accuracy of training data: ", round(sum(diag(tab1))/sum(tab1) * 100,2))
# Confusion Matrix & Misclassification Error - testing data
output <- compute(n2, testing_nn[,-5])
p2 <- output$net.result
pred2 <- ifelse(p2>0.5, 1, 0)
tab2 <- table(Prediction = pred2, Actual = testing_nn$Loan_Status)
tab2
paste0("Misclassification Error of testing data: ", round(100 - sum(diag(tab2))/sum(tab2)*100,2))
paste0("Accuracy of testing data: ", round(sum(diag(tab2))/sum(tab2)*100,2))
```

From above, we observe our misclassification error and accuracy for test and training data. It appears that this model has performed as well as the prior model and, if we consider model simplicity as well, may be the strongest.

## NN Model Evaluation

To clarify, we revisit performance statistics for the best model from each neural network approach:

```{r, eval = T, echo = F, message = F, warning = F, fig.height = 8, fig.width = 10}
#Create Kable table to succinctly summarize NN model results
Model <- c('1', '2', '3')
Method <- c('nnet', 'keras', 'neural net')
Var_Num <- c(5, 5, 1)
Train_Acc <- c(0.843, 0.839, 0.810)
Test_Acc <- c(0.825, 0.871, 0.871)
output <- cbind(Model, Method, Var_Num, Train_Acc, Test_Acc)
output %>%
  kbl(caption = "Neural Network Model Comparison") %>%
  kable_minimal() %>%
  kable_styling(latex_options = "hold_position")
```

***

# Back Comparison

At this point, we proceed to compare our neural network models against the performance metrics of a decision tree, random forest, logistic regression, XG boost, and K means model ...

## DT

```{r}
#Split data into training and testing sets
set.seed(123)
sample_data = sample.split(loan, SplitRatio = 0.75)
train_data <- subset(loan, sample_data == TRUE)
test_data <- subset(loan, sample_data == FALSE)
```

```{r}
#Class Method
prop.table(table(loan$Loan_Status))
DT_fit <- rpart(Loan_Status~., data = loan, method = 'class')
rpart.plot(DT_fit, extra = 106)
```
```{r}
predict_unseen <-predict(DT_fit, loan, type ='class')
table_mat <-table(loan$Loan_Status, predict_unseen)
table_mat
accuracy_Test <-sum(diag(table_mat))/sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))
```
```{r}
# Confusion matrix and misclassification error rate for training data
tab <- table(Predicted = predict(tree), Actual = train_data$Loan_Status)
print(tab)
print(paste('Misclassification error for training data', round(1 - sum(diag(tab))/sum(tab),3)))

# Confusion matrix and misclassification error rate for testing data
testPred <- predict(tree, test_data)
testtab <- table(Predicted = testPred, Actual = test_data$Loan_Status)

print(testtab)
print(paste('Misclassification error for testing data', round(1 - sum(diag(testtab))/sum(testtab),3)))
```

## RF

To prepare our data for Random Forest modeling, we convert variables of type factor to numeric, remove the `Gender` and `Dependents` variables from consideration, and change variables `Education` and `Property_Area` from categorical to numerical type.

We'll exclude `Gender` and `Dependents` from consideration because of their weak correlation with `Loan_Status`. This was proved from feature ranking and selection we did in the beginning of this analysis using the **Boruta** function. 

```{r}
# Remove Gender, Dependents
loan_RF <- subset(loan, select = -c(Gender, Dependents))
# Convert Credit_History to numeric type
loan_RF$Credit_History <- as.numeric(loan_RF$Credit_History)
# Change Variables values
loan_RF$Education <- ifelse(loan_RF$Education=="Graduate", 1, 0)
loan_RF$Married <- ifelse(loan_RF$Married=="Yes", 1, 0)
loan_RF$Self_Employed <- ifelse(loan_RF$Self_Employed=="Yes", 1, 0)
if(loan_RF$Property_Area=="Semiurban")
  {
    loan_RF$Property_Area <- 2
} else if(loan_RF$Property_Area=="Urban"){
    loan_RF$Property_Area <- 1
} else {
    loan_RF$Property_Area <- 0
}
head(loan_RF) #verify output
```
75% of our data will be training data and 25% will be testing data.

```{r}
set.seed(1247)
ind <- sample(2, nrow(loan_RF), replace = TRUE, prob = c(0.75, 0.25))
train_RF <- loan_RF[ind == 1, ]
test_RF <- loan_RF[ind == 2, ]
```

RF model will be created based on the important factors such as `Credit_History`, `Applicant_Income`, `CoapplicantIncome`, and `Loan_Amount`. 

```{r}
set.seed(102)
for (i in 1:10)
{
rf = randomForest(Loan_Status ~ Credit_History + LoanAmount + CoapplicantIncome + ApplicantIncome, data = train_RF, mtry = i)
err <- rf$err.rate
oob_err <- err[nrow(err), "OOB"]
print(paste("For mtry : ", i , "OOB Error Rate : ", round(oob_err, 4)))
}
rf_2 = randomForest(Loan_Status ~ Credit_History + LoanAmount + CoapplicantIncome + ApplicantIncome , data = train_RF, mtry = 1)
print(rf_2)
```

## Logistic Regression

## XG Boost

## K means


LEFT OFF HERE: add Feature Selection, Confusion Matrix, etc. Create Kable table for NN models. Compare and interpret. Select the best model. Proceed to compare this model vs. best performing others (RF, DT, XGB ...).
© 2021 GitHub, Inc.