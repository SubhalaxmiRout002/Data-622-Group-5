---
title: "DATA622_HW3"
author: "Group 5"
date: "`r Sys.Date()`" # Due 4/9/2021
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

## Authorship

**Group 5:** 

* Don (Geeth) Padmaperuma,
* Subhalaxmi Rout, 
* Isabel Ramesar, and
* Magnus Skonberg

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r library, comment=FALSE, warning=FALSE, message=FALSE }
library(tidyverse)
library(caret)
library(palmerpenguins)
library(e1071) 
library(caTools)
library(ggplot2)
library(GGally)
library(ggplot2) 
library(MASS) 
library(mvtnorm)
library(class)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(gbm)
library(mice)
library(plyr)
library(gridExtra)
library(kableExtra)
library(gbm)
library(xgboost)      # a faster implementation of gbm
```

# Background

The purpose of this assignment was to explore classification via K-nearest neighbors, Decision Trees, Random Forests, and Gradient Boosting.

## Classification

Classification is a supervised machine learning technique whose main purpose is to identify the category/class of provided input data.  The model that is generated is trained using labeled data (hence the label "supervised") and then the trained model is used to predict our discrete output. 

## Our Approach

First, we're going to predict `species` of penguin using the KNN algorithm.

Then we're going to compare and contrast `loan approval status` prediction accuracy for Decision Trees, Random Forests and Gradient Boosting.

................................................................................


# Palmer Penguins Data

Being that we've worked with the penguins dataset twice before, we perform *light EDA* to re-familiarize ourselves with the data prior to applying the KNN algorithm to it.

We load in the data, pre-process it, verify the first 6 observations, and utilize the built-in glimpse() function to gain insight into the dimensions, variable characteristics, and value range:

```{r}
#Load and tidy data 
penguin_measurements <- penguins %>% drop_na() %>%
    dplyr::select(species, bill_length_mm, bill_depth_mm,
                  flipper_length_mm, body_mass_g)

head(penguin_measurements%>% as.data.frame())
glimpse(penguin_measurements)
```

Once we've dropped NA values and selected pertinent variables, we end up with a 333 observation x 5 variable data frame with:

* `species`, a categorical variable of type factor, as our dependent variable and
* `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`, all quantitative variables of type dbl or int, as our independent variables.


# 1. K-Nearest Neighbors (KNN)

The KNN algorithm hinges on the idea that similar data will be near one another. When applying KNN, the distance between data points are used to classify data. Those nearer one another are "batched" together. The amount of batches we see and the distance between points is determined by the k value we select:

* Smaller k --> fewer batches and larger distance between "like" points.
* Larger k --> more batches and smaller distance between "like" points. 

Due to the simplicity of calculations involved and ease of interpretability of the result, KNN is a popular classifier. For our purposes, we're going to apply the K-nearest neighbor algorithm to the Palmer penguins dataset to predict the `species` variable.

We perform an 80-20 split, center and scale our independent variables via built-in scale() function, and fit the KNN model to our training dataset:

```{r}
# Splitting data into train and test data 
set.seed(123) 
training.individuals <- penguin_measurements$species %>%
    createDataPartition(p = 0.8, list = FALSE) 

train_cl <- penguin_measurements[training.individuals, ] 
test_cl <- penguin_measurements[-training.individuals, ] 

# Feature Scaling 
train_scale <- scale(train_cl[, 2:5]) 
test_scale <- scale(test_cl[, 2:5]) 

# Fitting KNN Model to training dataset 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 1) 

#classifier_knn #verify output
```

Performing an 80-20 split allocates 80% of our data for training the model and 20% of our data for testing it. Whereas applying the scale() function centers and scales our independent variables to reduce bias and improve predictive accuracy. *Although, we start with k=1 (neighbors) for our KNN model fit here. We'll vary this value later to interpret its impact on predictive accuracy.*

Once our model has been fit, we assess its accuracy via confusion matrix :

```{r}
# Confusion Matrix 
cm <- table(test_cl$species, classifier_knn) 
cm 

# Model Evaluation - Choosing K Calculate out of Sample error 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy =', 1-misClassError)) 
```

In the confusion matrix above, rows represent actual values while columns represent predicted values. With this in mind, we see that our test set results are:

* True Positive Result (TPR): 61 / 65 = 93.8%
* False Positive Result (FPR): 4 / 65 = 6.2%

Although our KNN classifier mis-predicted an Adelie as a Chinstrap and 3 Chinstraps as Adelies, it predicted Gentoo with 100% accuracy and produced a relatively favorable **93.8% accuracy**.

Let's explore the impact of increasing our k value to 3:

```{r}
# K = 3 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 3) 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy (k=3):', 1-misClassError)) 


```

We don't visualize the output with a confusion matrix and instead calculate the sample error and just subtract this value from 1 to produce a predictive **accuracy of 95.4%**. A 1.6% improvement over our KNN classifier with k=1.

We, once again, explore the impact of increasing our k value. This time we increase it to 15:

```{r}
# K = 15 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 15) 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy (k=15):', 1-misClassError)) 
```

With k=15 **our predictive accuracy climbs to 96.9%.** A 1.5% improvement over our KNN classifier with k=3. In our case, each increase in k value improved our predictive accuracy. This is not always the case though ... 

When choosing k values, smaller values are generally less computationally expensive *yet* they're also noisier and less accurate. Larger values, on the other hand, can result in smoother decision boundaries and a lower variance *yet* they increase the bias and processing demands. 

Thus, more often than not we seek the "sweet spot". A k value that's not too large and not too small. Our choice in value is thus impacted by the number of observations, as well as the characteristics of the data we're classifying.

................................................................................


# Loan Approval Data

Being that we haven't worked with the loan approval dataset before, our exploratory data analysis (EDA) will be more in-depth. The depth will allow for greater understanding of the data at hand prior to applying a Decision Tree, Random Forest, and Gradient Boosting model to it.

We load in the data, replace empty strings with NAs, and observe the first 6 observations of our dataset:

```{r}
#Load in data
loan <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/Loan_approval.csv", stringsAsFactors = TRUE)

loan[loan==""] <- NA #replace empty strings with NAs
#head(loan) #verify 1st 6 observations
```

The head() function provides some context regarding the format of our data. R's built-in glimpse() and summary() functions provide further insight:

```{r}
#Light EDA
glimpse(loan)
summary(loan)
```


We're dealing with a 614 observation x 13 variable data frame with:

* `Loan_Status`, a categorical, character-based variable, as our dependent variable,
* `ApplicantIncome`, `CoApplicantIncome`,`LoanAmount`, `Loan_Amount_Term`, and `Credit_History`, all quantitative variables of type dbl or int, as independent variables, and
* `Loan_ID`, `LoanGender`, `Married`, `Dependents`, `Education`, `Self_Employed`, `Property_Area`, and `Loan_Status`, all categorical, character-based variables, as independent variables.

From the above output, we also get an idea of proportions for our variables of type factor (ie.`Gender`: 489 male, 112 female applicants) which we can explore in greater depth later.

Of the above variables, we can see that `Loan_ID` does not appear to provide much insight. We'll drop this variable, explore a clearer visualization of NA counts and then deal with our NA values: 

```{r}
loan <- subset(loan, select = -c(1) ) #drop Loan_ID from consideration

colSums(is.na(loan)) #visualize NA counts
```

7 / 12 variables have NA values and 3 of these variables have more than 20 NA values (a relatively significant margin).

We *can* drop these values but dropping values means losing valuable observations and thus we elect to impute instead. From the mice library, we impute using the **pmm** method (predictive mean matching):

```{r}
#relabel Dependents "3+" value as "3" so that we can impute values
loan$Dependents <- revalue(loan$Dependents, c("3+"="3"))

#apply predictive mean matching to loan data
loan <- mice(loan, m = 1, method = "pmm", seed = 500)
loan <- mice::complete(loan, 1)
```

We re-assign the "3+" value of the `Dependents` variable to provide consistent leveling and enable **pmm** and then we actually apply **pmm**.

Predictive mean matching calculates the predicted value for our target variable, and, for missing values, forms a small set of “candidate donors” from the complete cases that are closest to the predicted value for our missing entry. Donors are then randomly chosen from candidates and imputed where values were once missing. *To apply pmm we assume that the distribution is the same for missing cells as it is for observed data, and thus, the approach may be more limited when the % of missing values is higher.*

Once we've imputed missing values into our loan dataset and returned the data in proper form, we verify whether our operation was successful:

```{r}
#verify absence of NA values in the dataset
colSums(is.na(loan))
```

Imputation was a success and thus our data pre-processing has been completed. We can proceed with our exploratory data analysis (EDA).

To identify features that carry promise vs. those that may not, we consider a correlation matrix for our numeric variables: 

```{r message=FALSE}
#Correlation matrix for numeric variables
library(corrplot)

#Loan Approved correlation
loan_corr_y <- loan %>%
    filter(Loan_Status == "Y") %>%
    select_if(is.numeric) %>%
    cor()

corrplot(loan_corr_y, title="Loan Approved",type = "lower")

#Loan Rejected correlation
loan_corr_n <- loan %>%
    filter(Loan_Status == "N") %>%
    select_if(is.numeric) %>%
    cor()
corrplot(loan_corr_n, title="Loan Rejected", type = "lower", diag = T)
```

When we consider loan approval and rejection, `LoanAmount` and `ApplicantIncome` have a relatively strong correlation. For loan rejection, `LoanAmount` also appears to have moderate correlation with `CoapplicantIncome`. Remaining features do not have strong correlation and thus we'll only note the aforementioned relationships for consideration for *possible* feature removal later. 

We move on to explore histograms for our numeric variables:

```{r eda, eval = T}
#Histograms for all variables
loan %>%
    keep(is.numeric) %>%
    subset(select = -c(4,5)) %>% #drop CreditHistory, Loan_Amount_Term
    gather() %>% 
    ggplot(aes(value)) +
        facet_wrap(~ key, scales = "free", ncol=1) +
        geom_histogram(bins=90,color="darkblue", fill="lightblue")

```
From the above figures we observe that:

* `ApplicantIncome` and `LoanAmount` appear to be right skewed normal with a number of noteworthy outliers, and
* `CoapplicantIncome` has a peak at 0 joined with an otherwise right skewed normal distribution.

Scaling is not necessary for Decision Trees or Random Forests but may prove useful for Gradient Descent. Thus, we won't act on normalization at this point, we'll just note the point for future consideration.

Next, we explore our categorical variables:

```{r, message=FALSE}
#convert CreditHistory to type factor
loan$Credit_History <- factor(loan$Credit_History)
#levels(loan$Credit_History) #verify

#MS: if there's a way to facet wrap this / automate, would be preferred:

#Histograms for all categorical variables
##Gender
p1 <- loan %>% 
    dplyr::select(1,12) %>% #dplyr::select(1:5,11:12)
    group_by(,Loan_Status) %>%
    #gather() %>%
    count() %>%
    
    ggplot(aes(x=Gender, y=freq, fill=Loan_Status)) + 
        #facet_wrap(~ key) +
        geom_bar(stat='identity', position="stack")

##Married
p2 <- loan %>% dplyr::select(2,12) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Married, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

##Dependents
p3 <- loan %>% dplyr::select(3,12) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Dependents, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

##Education
p4 <- loan %>% dplyr::select(4,12) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Education, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

##Self_Employed
p5 <- loan %>% dplyr::select(5,12) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Self_Employed, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

##Property_Area
p6 <- loan %>% dplyr::select(11,12) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Property_Area, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3, ncol = 2)

```

From the above figures we can extend the following observations:

* **males outnumber females** and **non self employed outnumber self employed** on a 5:1 basis, **married outnumber non married** on a 2:1 basis, **those without dependents make up a majority**, those that have **graduated make up a majority**, and those with **properties in semi-urban area make up a slight majority** of loan applications for the data under our consideration,
* with regard to loan approval, the only clearer takeaway is that it appears that **those in semi-urban areas are approved at a greater rate than those in rural and urban areas**.

While the visualization of our six categorical variables distributions with regard to our dependent variable is enlightening, it doesn't appear to provide clear indication regarding one vs. another's predictive capabilities. There's the possibility that many of these variables will prove impertinent and thus we may be justified in their exclusion from our models later in the process.

With a relatively thorough exploratory analysis under our belt, we move on to the building and assessment of our Decision Tree, Random Forest, and Gradient Descent models.

................................................................................


# 2. Decision Trees

Decision trees build classification or regression models in the form of a tree structure. Datasets are broken into smaller and smaller subsets along chosen parameters, as the associated decision tree is developed. 

The result is a tree with nodes and branches. Nodes denote a split point based on the attribute in question while branches denote the corresponding outcome. We start at a "root node", terminate at "leaf nodes", and use corresponding lead nodes to provide proportions regarding resulting class labels.

Due to the ease of interpretability, decision trees are a popular early classifier. For our purposes, we're dealing with a categorical dependent variable, and will build a Decision Tree model from the loan approval dataset to provide `Loan_Status` predictions (ie. Approve or Reject).

**Thoroughly document the reasoning behind the steps we take.**

Before creating our model, we first verify the proportion of loans that are approved vs. rejected: 

```{r}
# size of dataset
#dim(loan) #MS: step completed during EDA

# Count for Loan approved or not
table(loan$Loan_Status)
```

The loans under consideration are approved at more than a 2:1 ratio.

Data Preparation for Decision Tree model:

* Some variables type have factor,convert factor to numeric.
* Remove Gender variable from data, because loan approval does not depend on gender
* Change variables  Education and Property_Area from Categorical to numeric 
  
  + Education: Graduate = 1 and Not Graduate = 0
  + Married: Yes = 1 and No = 0
  + Self_Employed:  Yes = 1, Urban = 1, No = 0
  
##### DT Model 1

This model consists of all attributes excluding Gender. 

```{r}
# Remove Gender, Married, Self_Employed
loan_new <- subset(loan, select = -c (Gender))

##MS: why no gender?

# Convert Dependents, Credit_History numeric type
loan_new$Dependents <- as.numeric(loan_new$Dependents)
loan_new$Credit_History <- as.numeric(loan_new$Credit_History)

# Change Variables values
loan_new$Education <- ifelse(loan_new$Education=="Graduate", 1, 0)
loan_new$Married <- ifelse(loan_new$Married=="Yes", 1, 0)
loan_new$Self_Employed <- ifelse(loan_new$Self_Employed == "Yes", 1, 0)

if(loan_new$Property_Area=="Semiurban")
  {
    loan_new$Property_Area <- 2
} else if(loan_new$Property_Area=="Urban"){
    loan_new$Property_Area <- 1
} else {
    loan_new$Property_Area <- 0
}

```

Split data in to train and test.

```{r}
#Split data into training and testing sets
set.seed(123)
sample_data = sample.split(loan_new, SplitRatio = 0.75)
train_data <- subset(loan_new, sample_data == TRUE)
test_data <- subset(loan_new, sample_data == FALSE)
```

```{r}
set.seed(144)
#Class Method
# prop.table(table(loan$Loan_Status))
# fit <- rpart(Loan_Status~., data = loan, method = 'class')
# rpart.plot(fit, extra = 106)


# Plot tree
binary.model <- rpart(Loan_Status ~ ., data = train_data, cp = .02)
rpart.plot(binary.model)

# Fit model 
tree <- ctree(Loan_Status ~ ., data = train_data)
```


```{r}
# Misclassification Error with Train data
tab <- table(Predicted = predict(tree), Actual = train_data$Loan_Status)
print(tab)
print(paste('Misclassification Error with Train data', round(1 - sum(diag(tab))/sum(tab),3)))

# Misclassification Error with Test data
testPred <- predict(tree, test_data)
testtab <- table(Predicted = testPred, Actual = test_data$Loan_Status)
print(testtab)
print(paste('Misclassification Error with Test data', round(1 - sum(diag(testtab))/sum(testtab),3)))

# fit <- rpart(Loan_Status~., data = train_data)
# fit <- rpart(Loan_Status~., data = loan, method = 'class')
# predict_unseen <-predict(fit, loan, type ='class')
# table_mat <-table(loan$Loan_Status, predict_unseen)
# table_mat
# accuracy_Test <-sum(diag(table_mat))/sum(table_mat)
# print(paste('Accuracy for test', accuracy_Test))
```

From the Decision tree shown above we can conclude that Credit_History is the most important factor when deciding if someone will be approved for a loan or not.

* Train dataset accuracy : `r (1 - 0.177) * 100`, error rate: `r 0.177 * 100`
* Test dataset accuracy : `r (1 - 0.198) * 100`, error rate:`r 0.198 * 100`


##### Model Evaluation


The library caret has a function to make prediction. Use the prediction to compute the confusion matrix and see the accuracy score and other matrices.

```{r}
confusionMatrix(testPred, test_data$Loan_Status)
```


Decision Tree model shows an accuracy of 80.24 %.

Let's have a tabular view of DT model and its matrices.


```{r}

DT_Model <- confusionMatrix(testPred, test_data$Loan_Status)$byClass

tabview <- data.frame(DT_Model)

tabview %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```
Comment on this.

# 3. Random Forests

Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting, and easy interpretability. RF can be used for classification or regression. It can deal with Large number of features.

More features are better for the model, we will exclude the Gender variable because logically it is not correct that for Female loan approval will be less and more for Male. Most of data has zero dependants, so Dependents are excluded. 

Data Transformation for RF:

* Convert Factor variables to numeric
* Change variables  Education and Property_Area from Categorical to numeric 
  
  + Education: Graduate = 1 and Not Graduate = 0
  + Property_Area: Semiurban = 2, Urban = 1, Rural = 0
  + Married: Yes = 1, No = 0
  + Self_Employed: Yes = 1, No = 0

```{r}
loan_RF <- subset(loan, select = -c(Gender, Dependents))

# Convert Dependents, Credit_History numeric type

#loan_RF$Dependents <- as.numeric(loan_RF$Dependents)
loan_RF$Credit_History <- as.numeric(loan_RF$Credit_History)

# Change Variables values
loan_RF$Education <- ifelse(loan_RF$Education=="Graduate", 1, 0)
loan_RF$Married <- ifelse(loan_RF$Married=="Yes", 1, 0)
loan_RF$Self_Employed <- ifelse(loan_RF$Self_Employed=="Yes", 1, 0)



if(loan_RF$Property_Area=="Semiurban")
  {
    loan_RF$Property_Area <- 2
} else if(loan_RF$Property_Area=="Urban"){
    loan_RF$Property_Area <- 1
} else {
    loan_RF$Property_Area <- 0
}

head(loan_RF)

```

Split data in to train and test sets. 


```{r}
set.seed(1247)

ind <- sample(2, nrow(loan_RF), replace = TRUE, prob = c(0.75, 0.25))
train_RF <- loan_RF[ind == 1, ]
test_RF <- loan_RF[ind == 2, ]

dim(train_RF)
dim(test_RF)

```

Random Forest Model building

Helps with feature selection based on importance and avoids overfitting.

Steps:

* Draw ntree bootstrap samples
* For each bootstrap sample, grow un-pruned tree by choosing best split based on random sample of mtry predictors at each node
* Predict test data using majority votes for classification and average for regression based on ntrees

##### Random Forest Model 1

Create the first model with all variables except gender and Dependents.

```{r}
library(randomForest)

set.seed(222)

for (i in 1:10)
{
rf = randomForest(Loan_Status ~ . , data = train_RF, mtry = i)
err <- rf$err.rate
oob_err <- err[nrow(err), "OOB"]
print(paste("For mtry : ", i , "OOB Error Rate : ", round(oob_err, 4)))
}

```

From above OOB Error Rate, mtry = 2 shows less error. We will set mtry = 2 for RF model.

```{r}
set.seed(35)
rf_1 = randomForest(Loan_Status ~ . , data = train_RF, mtry = 2)
print(rf_1)

#Importance of each predictor
print(importance(rf_1, type=2))
```

From the random forest shown above we can conclude that Credit_History, Applicant_Income, CoapplicantIncome, and Loan_Amount are the most important factors when deciding if someone will be approved for a loan or not. The model has a 18.04% error which means we can predict with `r 100 - 18.04`% accuracy.

##### Random Forest Model 2

This model we select only important factors such as Credit_History, Applicant_Income, CoapplicantIncome, and Loan_Amount.


```{r}
set.seed(102)

for (i in 1:10)
{
rf = randomForest(Loan_Status ~ Credit_History + LoanAmount + CoapplicantIncome + ApplicantIncome, data = train_RF, mtry = i)
err <- rf$err.rate
oob_err <- err[nrow(err), "OOB"]
print(paste("For mtry : ", i , "OOB Error Rate : ", round(oob_err, 4)))
}

rf_2 = randomForest(Loan_Status ~ Credit_History + LoanAmount + CoapplicantIncome + ApplicantIncome , data = train_RF, mtry = 1)
print(rf_2)

```


##### Prediction

We will do the model evaluation, apply RF model 1 and model 2 with the test dataset to see the accuracy and other matrices. 


```{r}
rf_predict_1 <- predict(rf_1, newdata = test_RF)
confusionMatrix(rf_predict_1, test_RF$Loan_Status)

rf_predict_2 <- predict(rf_2, newdata = test_RF)
confusionMatrix(rf_predict_2, test_RF$Loan_Status)

```

We have an accuracy of 80 % for the first model and 81.82 % for the second model.

Let's have a tabular view of RF model 1 and Model 2 matrix comparison.

```{r}

RF_Model_1 <- confusionMatrix(rf_predict_1, test_RF$Loan_Status)$byClass
RF_Model_2 <- confusionMatrix(rf_predict_2, test_RF$Loan_Status)$byClass

tabularview <- data.frame(RF_Model_1, RF_Model_2)

tabularview %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```


# 4. Gradient Boosting

Gradient Boosting is a machine learning algorithm, used for both classification and regression problems. Gradient boosting works by building simpler (weak) prediction models sequentially where each model tries to predict the error left over by the previous model. A weak model is one  that does slightly better than random predictions. Boosting works on the principle of improving mistakes of the previous learner through the next learner.

In boosting, each new tree is a fit on a modified version of the original data set. Gradient Boosting trains many models in a gradual, additive and sequential manner. 

Our approach: 

* **Pre-process data**: normalize independent numeric variables, drop impertinent variables, convert categorical variables to numeric, binary scale, convert dependent variable to factor type
* **Train-test split**: we partition our data with a 75% train, 25% test split
* **Matrix conversions**: we convert our data frames to matrices as required to be read in to XGBoost
* **Train model**: initialize parameters and play with variable values to observe the effect
* **Optimize model**: using error values and feature importance rankings, we optimize our model for the highest predictive accuracy
* **Output statistics**: verify the accuracy of our model using a confusion matrix and then using caret's built-in function.

```{r}
#Pre-process data
#Normalize independent numeric vars

#store sum of ApplicantIncome and CoapplicantIncome in ApplicantIncome
loan$ApplicantIncome <- loan$ApplicantIncome + loan$CoapplicantIncome

#calculate skewness prior to normalization
skewness(loan$ApplicantIncome, na.rm = TRUE)
#skewness(loan$CoapplicantIncome, na.rm = TRUE)
skewness(loan$LoanAmount, na.rm = TRUE)

#normalization: account for outliers with log transform
loan$ApplicantIncome <- log10(loan$ApplicantIncome)
#loan$CoapplicantIncome <- log10(loan$CoapplicantIncome)
loan$LoanAmount <- log10(loan$LoanAmount)

#calculate skewness after normalization
skewness(loan$ApplicantIncome, na.rm = TRUE)
#skewness(loan$CoapplicantIncome, na.rm = TRUE) #produced NaN - dealt with via summing
skewness(loan$LoanAmount, na.rm = TRUE)
```

Based on the improved skewness values our normalization was a success and these (2) independent, numeric variables are now prepared to be included in our Gradient Boosting model.

```{r}
#2. drop impertinent variables, convert to numeric, and change variable values
loan_GB <- subset(loan, select = -c(Gender, Property_Area, CoapplicantIncome)) #since CoapplicantIncome is incorporated with ApplicantIncome

# Convert Dependents, Credit_History numeric type
loan_GB$Dependents <- as.numeric(loan_GB$Dependents)
loan_GB$Credit_History <- as.numeric(loan_GB$Credit_History)

# Change Variables values
#loan_GB$Gender <- ifelse(loan_GB$Gender=="Male", 1, 0)
loan_GB$Education <- ifelse(loan_GB$Education=="Graduate", 1, 0)
loan_GB$Married <- ifelse(loan_GB$Married=="Yes", 1, 0)
loan_GB$Self_Employed <- ifelse(loan_GB$Self_Employed=="Yes", 1, 0)

#3. convert Loan_Status to type factor
loan_GB$Loan_Status <- ifelse(loan_GB$Loan_Status=="Y", 1, 0)
#loan$Loan_Status <- as.factor(loan$Loan_Status)

head(loan_GB) #verify all numeric inputs
```

At this point pre-processing of our data is complete and we've verified that we will indeed be feeding our model all numeric inputs. Next, we partition our data:

```{r}
#Partition data
set.seed(1234)

#loan_GB
ind <- sample(2, nrow(loan_GB), replace = TRUE, prob = c(0.75, 0.25))
train_GB <- loan_GB[ind == 1, ]
test_GB <- loan_GB[ind == 2, ]

#dim(train_GB) #457 x 10
#dim(test_GB) #157 x 10
```

```{r}
#Create train, test matrices - one hot encoding for factor variables
library(Matrix)
trainm <- sparse.model.matrix(Loan_Status ~ ., data = train_GB)
#head(trainm)
train_label <- train_GB[,"Loan_Status"]
train_matrix <- xgb.DMatrix(data = as.matrix(trainm),label = train_label )

testm <- sparse.model.matrix(Loan_Status ~ ., data = test_GB)
test_label <- test_GB[,"Loan_Status"]
test_matrix <- xgb.DMatrix(data = as.matrix(testm),label = test_label )
```

```{r}
#Parameters
nc <- length(unique(train_label)) #number of classes
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = nc)
watchlist <- list(train = train_matrix, test = test_matrix)

#extreme Gradient Boosting Model
GB_model <- xgb.train(params = xgb_params,
                      data = train_matrix,
                      nrounds = 5, #run 100 iterations 1st then update based on test error value
                      watchlist = watchlist,
                      eta = 0.1) #inc eta value increased accuracy by 1

```

```{r}
#error plot
e <- data.frame(GB_model$evaluation_log)
plot(e$iter, e$train_mlogloss)
lines(e$iter, e$test_mlogloss, col = 'red')

#determine when test error was lowest
min(e$test_mlogloss) #0.456353 lowest error
e[e$test_mlogloss == 0.456353,] #5th iteration

#feature importance
imp <- xgb.importance(colnames(train_matrix), model=GB_model)
print(imp) #higher Gain means higher feature importance
```

```{r}
#prediction and confusion matrix from train data
p_train <- predict(GB_model, newdata = train_matrix)
pred_train <- matrix(p_train, nrow = nc, ncol = length(p_train)/nc) %>%
    t() %>% #matrix transpose
    data.frame() %>%
    mutate(label = train_label, max_prob = max.col(.,"last")-1)

tab_train <- table(Prediction = pred_train$max_prob, Actual = pred_train$label)
print(tab_train)
print(paste('Misclassification Error with Train data', round(1 - sum(diag(tab_train))/sum(tab_train),3)))

#prediction and confusion matrix from test data
p_test <- predict(GB_model, newdata = test_matrix)
pred_test <- matrix(p_test, nrow = nc, ncol = length(p_test)/nc) %>%
    t() %>% #matrix transpose
    data.frame() %>%
    mutate(label = test_label, max_prob = max.col(.,"last")-1)

tab_test <- table(Prediction = pred_test$max_prob, Actual = pred_test$label)
print(tab_test)
print(paste('Misclassification Error with Test data', round(1 - sum(diag(tab_test))/sum(tab_test),3)))
```

## XGBoost Model 2

Based on the weight of feature importance, we could disclude Loan_Amount_Term, Dependents, Self_Employed, Education, Married, and Gender. We revisit the model without these terms to observe the effect on accuracy:

```{r}
#2. drop impertinent variables, convert to numeric, and change variable values
loan_GB2 <- subset(loan, select = -c(CoapplicantIncome, Gender, Loan_Amount_Term, Dependents, Self_Employed, Education, Married, Property_Area))

# Convert Dependents, Credit_History numeric type
loan_GB2$Credit_History <- as.numeric(loan_GB2$Credit_History)

#3. convert Loan_Status to type factor
loan_GB2$Loan_Status <- ifelse(loan_GB2$Loan_Status=="Y", 1, 0)
#loan$Loan_Status <- as.factor(loan$Loan_Status)

#head(loan_GB2) #verify all numeric inputs
```

At this point pre-processing of our data is complete and we've verified that we will indeed be feeding our model all numeric inputs. Next, we partition our data:

```{r}
#Partition data
set.seed(1234)

#loan_GB

ind <- sample(2, nrow(loan_GB2), replace = TRUE, prob = c(0.75, 0.25))
train_GB2 <- loan_GB2[ind == 1, ]
test_GB2 <- loan_GB2[ind == 2, ]

dim(train_GB2) #457 x 4
dim(test_GB2) #157 x 4
```

```{r}
#Create train, test matrices - one hot encoding for factor variables
library(Matrix)
trainm2 <- sparse.model.matrix(Loan_Status ~ ., data = train_GB2) # rmv Loan_Status col (?)
#head(trainm2)
train_label2 <- train_GB2[,"Loan_Status"]
train_matrix2 <- xgb.DMatrix(data = as.matrix(trainm2),label = train_label2 )

testm2 <- sparse.model.matrix(Loan_Status ~ ., data = test_GB2)
test_label2 <- test_GB2[,"Loan_Status"]
test_matrix2 <- xgb.DMatrix(data = as.matrix(testm2),label = test_label2 )

#Parameters
nc2 <- length(unique(train_label2)) #number of classes
xgb_params2 <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = nc2)
watchlist2 <- list(train = train_matrix2, test = test_matrix2)

#extreme Gradient Boosting Model
GB_model2 <- xgb.train(params = xgb_params2,
                      data = train_matrix2,
                      nrounds = 20, #run 100 iterations 1st then update based on test error value
                      watchlist = watchlist2,
                      eta = 0.1) #inc eta value increased accuracy by 1


#error plot
e2 <- data.frame(GB_model2$evaluation_log)
plot(e2$iter, e2$train_mlogloss)
lines(e2$iter, e2$test_mlogloss, col = 'red')

#determine when test error was lowest
min(e2$test_mlogloss) #0.478216 lowest error
e2[e2$test_mlogloss == 0.478216,] #20th iteration

#feature importance
imp2 <- xgb.importance(colnames(train_matrix2), model=GB_model2)
print(imp2) #higher Gain means higher feature importance

#prediction and confusion matrix from test data
# p2 <- predict(GB_model2, newdata = test_matrix2)
# pred2 <- matrix(p2, nrow = nc2, ncol = length(p2)/nc2) %>%
#     t() %>% #matrix transpose
#     data.frame() %>%
#     mutate(label = test_label2, max_prob = max.col(.,"last")-1)
# 
# table(Prediction = pred2$max_prob, Actual = pred2$label)

#prediction and confusion matrix from train data
p_train2 <- predict(GB_model2, newdata = train_matrix2)
pred_train2 <- matrix(p_train2, nrow = nc2, ncol = length(p_train2)/nc2) %>%
    t() %>% #matrix transpose
    data.frame() %>%
    mutate(label = train_label2, max_prob = max.col(.,"last")-1)

tab_train2 <- table(Prediction = pred_train2$max_prob, Actual = pred_train2$label)
print(tab_train2)
print(paste('Misclassification Error with Train data', round(1 - sum(diag(tab_train2))/sum(tab_train2),3)))

#prediction and confusion matrix from test data
p_test2 <- predict(GB_model2, newdata = test_matrix2)
pred_test2 <- matrix(p_test2, nrow = nc2, ncol = length(p_test2)/nc2) %>%
    t() %>% #matrix transpose
    data.frame() %>%
    mutate(label = test_label2, max_prob = max.col(.,"last")-1)

tab_test2 <- table(Prediction = pred_test2$max_prob, Actual = pred_test2$label)
print(tab_test2)
print(paste('Misclassification Error with Test data', round(1 - sum(diag(tab_test2))/sum(tab_test2),3)))
```


The library caret has a function to make prediction. Use the prediction to compute the confusion matrix and see the accuracy score and other matrices.

```{r}

GB_Model_1 <- confusionMatrix(factor(pred_test$max_prob),factor(pred_test$label))$byClass
GB_Model_2 <- confusionMatrix(factor(pred_test2$max_prob),factor(pred_test2$label))$byClass

tabularview <- data.frame(GB_Model_1, GB_Model_2)

tabularview %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```

From the above error plot we can see that as our iterations increase, our training error decreases. Our testing error, on the other hand, initially decreases and then increases with each iteration.


# 5. Model Comparison (Decision Trees vs. Random Forests vs. Gradient Boosting)

We evaluate our models by applying **RF Model 1** and **RF Model 2** to the test data set. We utilize the **confusionMatrix** function from the caret library and present our statistics as a kable table to glean more insight regarding the comparative statistics between each model's performance:

```{r}

#Tabular view of RF model 1 and Model 2 matrix comparison

rf_predict_1 <- predict(rf_1, newdata = test_RF)
#confusionMatrix(rf_predict_1, test_RF$Loan_Status)
RF_Model_1 <- confusionMatrix(rf_predict_1, test_RF$Loan_Status)$byClass

rf_predict_2 <- predict(rf_2, newdata = test_RF)
#confusionMatrix(rf_predict_2, test_RF$Loan_Status)
RF_Model_2 <- confusionMatrix(rf_predict_2, test_RF$Loan_Status)$byClass

tabularview <- data.frame(RF_Model_1, RF_Model_2)

tabularview %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
```
The models share the same Sensitivity, Recall, Prevalence, and Detection Rates, but for every other metric (aside from Detection Prevalence) RF Model 2 outperforms RF Model 1. Based on the output statistics above **RF Model 2 is superior to RF Model 1** and will serve as our chosen Random Forest model.

For now, we note the Random Forest model's performance based on these classification metrics. Later, we'll provide an in-depth interpretation of these statistics vs. those of our Decision Tree and Gradient Boosting models.
................................................................................


# References

https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
https://guru99.com/r-decision-trees.html/
https://www.geeksforgeeks.org/decision-tree-for-regression-in-r-programming/
https://data-flair.training/blogs/r-decision-trees/
https://tutorialspoint.com/r/r_random_forest.htm/
https://statology.org/random-forest-in-r/
https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/ (normalization)
https://appsilon.com/r-xgboost/ (XGBoost)
http://rstudio-pubs-static.s3.amazonaws.com/368478_bf9700befeba4283a4640a9a1285af22.html (XGBoost)
https://www.youtube.com/watch?v=woVTNwRrFHE (XGBoost)
https://www.machinelearningplus.com/machine-learning/gradient-boosting/

