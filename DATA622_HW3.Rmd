--
title: "DATA622_HW3"
author: "Group 5 - Don (Geeth) Padmaperuma,Subhalaxmi Rout, Isabel R., Magnus Skonberg"
date: "4/9/2021"
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, comment=FALSE, warning=FALSE, message=FALSE }
library(tidyverse)
library(caret)
library(palmerpenguins)
library(e1071) 
library(caTools)
library(ggplot2)
library(GGally)
library(ggplot2) 
library(MASS) 
library(mvtnorm)
library(class)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(gbm)
```

# Penguin Data Set

## K-nearest neighbor (KNN)
1. Please use K-nearest neighbor (KNN) algorithm to predict the species variable. Please be sure to walk through the steps you took. (40 points)
```{r}
# Loading & Tidy data 
penguin_measurements <- penguins %>% drop_na() %>% dplyr::select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
head(penguin_measurements%>% as.data.frame())
glimpse(penguin_measurements)
```

```{r}
# Splitting data into train and test data 
set.seed(123) 
training.individuals <- penguin_measurements$species %>% createDataPartition(p = 0.8, list = FALSE) 
train_cl <- penguin_measurements[training.individuals, ] 
test_cl <- penguin_measurements[-training.individuals, ] 

# Feature Scaling 
train_scale <- scale(train_cl[, 2:5]) 
test_scale <- scale(test_cl[, 2:5]) 

# Fitting KNN Model to training dataset 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 1) 
classifier_knn 

# Confusion Matrix 
cm <- table(test_cl$species, classifier_knn) 
cm 

# Model Evaluation - Choosing K Calculate out of Sample error 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy =', 1-misClassError)) 

# K = 3 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 3) 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy =', 1-misClassError)) 


# K = 15 
classifier_knn <- knn(train = train_scale, 
					test = test_scale, 
					cl = train_cl$species, 
					k = 15) 
misClassError <- mean(classifier_knn != test_cl$species) 
print(paste('Accuracy =', 1-misClassError)) 
```

# Loan Approval Data Set

```{r}
# Loading & Tidy data 
data <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/Loan_approval.csv")
#data <-read.csv("C:\\Users\\iramesa\\Desktop\\MSDS\\DATA622\\Loan_approval.csv",header=T,stringsAsFactors=FALSE)
loan <- data %>% drop_na()
```

```{r loan data}
# Exploratory Analysis
glimpse(loan)
summary(loan)
```

## Decision Trees
2. Please use the attached dataset on loan approval status to predict loan approval using
Decision Trees. Please be sure to conduct a thorough exploratory analysis to start the
task and walk us through your reasoning behind all the steps you are taking. (40 points)

```{r}
#Split data into training and testing sets
set.seed(123)
sample_data = sample.split(loan, SplitRatio = 0.75)
train_data <- subset(loan, sample_data == TRUE)
test_data <- subset(loan, sample_data == FALSE)
```

```{r}
#Class Method
prop.table(table(loan$Loan_Status))
fit <- rpart(Loan_Status~., data = loan, method = 'class')
rpart.plot(fit, extra = 106)
```

```{r}
fit <- rpart(Loan_Status~., data = loan, method = 'class')
predict_unseen <-predict(fit, loan, type ='class')
table_mat <-table(loan$Loan_Status, predict_unseen)
table_mat
accuracy_Test <-sum(diag(table_mat))/sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))
```


## Random Forests
3. Using the same dataset on Loan Approval Status, please use Random Forests to predict on loan approval status. Again, please be sure to walk us through the steps you took to get to your final model. (50 points)
```{r}
#Create the forest
output.forest <-randomForest(Loan_Status~ Gender + Married + Dependents + ApplicantIncome + LoanAmount + Loan_Amount_Term + Credit_History + Property_Area, data = loan)

#View the forest results
print(output.forest)

#Importance of each predictor
print(importance(output.forest, type=2))
```
From the random forest shown above we can conclude that Credit_History, Applicant_Income and Loan_Amount are the most important factors when deciding if someone will be approved for a loan or not. The model has a 17.96% error which means we can predict with 82.04% accuracy.

```{r randomforestplot}
plot(output.forest)
varImpPlot(output.forest)
```

```{r randomforestprediction}
new <- data.frame(Gender='Male', Married='Yes', Dependents=0, ApplicantIncome=2000, LoanAmount=300, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban')
predict(output.forest, newdata =new)
```



## Gradient Boosting
4. Using the Loan Approval Status data, please use Gradient Boosting to predict on the loan approval status. Please use whatever boosting approach you deem appropriate; but please be sure to walk us through your steps. (50 points)

```{r}
#Separating training and test data
train=sample(1:261, size = 261)
loan.boost=gbm(Loan_Status ~ . ,data = loan[train,],distribution = "gaussian",n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
loan.boost

#Summary gives a table of Variable Importance and a plot of Variable Importance
summary(loan.boost)
```

```{r gradientboostplot}
plot(loan.boost, i="LoanAmount")
plot(loan.boost, i="ApplicantIncome")
```

```{r}
cor(loan$LoanAmount, loan$Credit_History)
cor(loan$ApplicantIncome, loan$Credit_History)
```

## Model Comparison (Decision Trees vs. Random Forests vs. Gradient Boosting)
5. Model performance: please compare the models you settled on for problem # 2 â€“ 4. Comment on their relative performance. Which one would you prefer the most? Why? (20 points)

# References
https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
https://guru99.com/r-decision-trees.html/
https://www.geeksforgeeks.org/decision-tree-for-regression-in-r-programming/
https://data-flair.training/blogs/r-decision-trees/
https://tutorialspoint.com/r/r_random_forest.htm/
https://statology.org/random-forest-in-r/
https://datascienceplus.com/gradient-boosting-in-r/
https://appsilon.com/r-xgboost/

