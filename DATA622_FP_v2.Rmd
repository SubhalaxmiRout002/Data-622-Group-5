---
title: "DATA622_FP"
author: "Group 5"
date: "`r Sys.Date()`" # Due 5/21/2021
output: 
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: paper
    highlight: monochrome
---

## Authorship

**Group 5:** 

* Don (Geeth) Padmaperuma,
* Subhalaxmi Rout, 
* Isabel Ramesar, and
* Magnus Skonberg

```{r setup, include=FALSE}
#Standardize chunk-knitting
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```

```{r library, include= FALSE}
#Load relevant libraries
library(tidyverse)
library(caret)
library(ggplot2) 
library(MASS) 
library(dplyr)
library(kableExtra)
library(plyr) #revalue 'Dependent'
library(mice) #pmm imputation
library(corrplot) #correlation matrix
library(Boruta) #Utilize Boruta for feature ranking and selection
library(gridExtra) #output plots via grid.arrange
library(car) #outlier handling

#Utilize customized functions

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

# Background

The purpose of our Final Project was to explore the application of Neural Networks to loan approval data to then back compare model performance with a variety of Classification algorithms (ie. KNN, DT, RF, GBM).

## Neural Networks

Neural networks form the basis of Deep Learning, an extension of Machine Learning, where algorithms are inspired by the structure of the human brain. They take in data, train themselves to recognize patterns therein, and then predict outputs for similar, unseen data.

```{r image='asis', out.width='600px'}
download.file(
    url='https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png',
    destfile='image1.jpg',
    mode='wb')
knitr::include_graphics(path='image1.jpg')

```

Neural networks are made up of layers of nodes. They contain an input layer, one or more hidden layers, and an output layer. Nodes are interconnected with associated weights and thresholds. When a node is above its specified threshold, the node is activated and data is sent to the next layer of the network. Otherwise, data is not fed forward.

The power of a neural network lies in its ability to fine-tune upon countless iterations. *Back-propagation* allows for continuous model accuracy improvement. Weights are adjusted based on the magnitude of error at the output layer, and continuous refinement allows for predictive accuracy improvements.

## Our Approach

We'll start by (re) exploring and preparing the loan dataset, progress to building our neural network model, and then compare and contrast `loan approval status` prediction accuracy for our neural network model vs. decision tree, random forest, and gradient boosting models.

***


# Loan Approval Data

A loan is when money is transferred from one party to another under the prospect that the lender (loan giver) will be repaid in full *with interest* by the lendee (loan receiver). 

The profit, for the lendor, comes from the interest they are paid by the lendee and thus, as a core part of their business model, it’s important for banks and credit companies alike to be able to depend upon the fact that their loan (and interest) will be repaid in full.

With this motivation in mind, we (re) explore and prepare our loan approval dataset in order to construct a more precise neural network model (later). Being that we've explored this data before, we build upon the core takeaways of our past exploration while simultaneously pushing the bounds of our understanding to a deeper level. *Rudimentary (early) EDA steps will be summarized and/or excluded from the write up and included in the Appendix in favor of output that provides greater context and insight.*

Prior to commencing EDA, we revisit the corresponding data directory:

* `LoanID`: unique loan ID
* `Gender`: applicant gender (Male/Female)
* `Married`: applicant marriage status (Yes/No)
* `Dependents`: number of dependents for applicant (0, 1, 2, 3+)
* `Education`: applicant college education status (Graduate / Not Graduate)
* `Self_Employed`: applicant self-employment status (Yes/No)
* `ApplicantIncome`: applicant income level
* `CoapplicantIncome`: co-applicant income level (if applicable)
* `LoanAmount`: loan amount requested (in thousands)
* `Loan_Amount_Term`: loan term (in months)
* `Credit_History`: credit history meets guidelines (1/0)
* `PropertyArea`: property location (Urban/Semi Urban/Rural)
* `Loan_Status`: loan approved (Yes/No). **target variable**

***


# Data Exploration & Preparation

To start, we load in our data, replace empty strings with NAs, observe the first 6 observations of our dataset to refamiliarize ourselves with the format of our data and then use R's built-in glimpse() and summary() functions to revisit data types and value ranges.

```{r, include=F}
#Load in data
loan <- read.csv("https://raw.githubusercontent.com/SubhalaxmiRout002/Data-622-Group-5/main/Final_Project/Loan_approval.csv", stringsAsFactors = TRUE)

loan[loan==""] <- NA #replace empty strings with NAs
#head(loan) #verify 1st 6 observations

#Light EDA
glimpse(loan)
summary(loan)

```

We're dealing with a 614 observation x 13 variable dataframe with `Loan_Status` as our dependent, categoric variable, `ApplicantIncome`, `CoApplicantIncome`,`LoanAmount`, `Loan_Amount_Term`, and `Credit_History` as independent, numeric variables, and `Loan_ID`, `LoanGender`, `Married`, `Dependents`, `Education`, `Self_Employed`, `Property_Area`, and `Loan_Status` as independent, categoric variables.

From above, we extend that `Loan_ID` can be dropped, `ApplicantIncome` and `CoApplicantIncome` can be combined to create a `TotalIncome` variable, and observations with a "3+" label in `Dependents` should be re-labelled as "3" so that data follows a consistent format and imputation can be performed as a next step.

## NA Values

We pre-process our data (as described above), visualize and handle NA values:

```{r}
#Pre-process dataset for easier interpretation
loan <- subset(loan, select = -c(1) ) #drop Loan_ID from consideration
loan$TotalIncome <- loan$CoapplicantIncome + loan$ApplicantIncome #create TotalIncome variable
loan <- subset(loan, select = -c(6,7) ) #drop CoapplicantIncome and ApplicantIncome
loan$Dependents <- revalue(loan$Dependents, c("3+"="3")) #relabel Dependents "3+" value as "3"

#Visualize NA counts
colSums(is.na(loan)) 

```

```{r, include=F}
#Handle NAs: apply predictive mean matching to loan data
loan <- mice(loan, m = 1, method = "pmm", seed = 500)
loan <- mice::complete(loan, 1)

```

We re-assign the "3+" value of the `Dependents` variable to provide consistent leveling and enable **pmm** (predictive mean matching). Predictive mean matching calculates the predicted value for our target variable, and, for missing values, forms a small set of “candidate donors” from the complete cases that are closest to the predicted value for our missing entry. Donors are then randomly chosen from candidates and imputed where values were once missing. *To apply pmm we assume that the distribution is the same for missing cells as it is for observed data, and thus, the approach may be more limited when the % of missing values is higher.*

Once we've imputed missing values, we verify whether our operation was successful:

```{r}
#verify absence of NA values in the dataset
colSums(is.na(loan))

```

Imputation was a success and data pre-processing has been completed. From this point we proceed to the observance of feature correlation.

## Correlation and Variable Importance

To identify feature correlation - how strongly independent variables are related to one another and how strongly these variables relate to our dependent variable (`Loan_Status`), we consider a correlation matrix with a threshold of 0.3: 

```{r}
#Utilize custom-built correlation matrix generation function
plot_corr_matrix(loan, 0.3)

```

From the correlation matrix we can extend that:

* `Credit_History` is our strongest predictor / strongly correlated with `Loan_Status`, and
* `Gender` and `Married`, `Married` and `Dependents`, `LoanAmount` and `TotalIncome` appear to be correlated with one another and indicative that multicollinearity may be a concern for our data.

We varied the threshold value for our correlation matrix and found that, aside from `Credit_History`, our other independent variables were relatively poor predictors of `Loan_Status`, making it worth exploring variable importance:

```{r, comment=FALSE, warning=FALSE, message=FALSE, fig.height = 8, fig.width = 10}
#NOTE: COMMENTED OUT BELOW DUE TO LONG COMPILATION TIME. UNCOMMENT BEFORE FINAL SUBMISSION.

# Perform Boruta search
#boruta_output <- Boruta(Loan_Status ~ ., data=na.omit(loan), doTrace=0, maxRuns = 1000)
#Get significant variables including tentatives
#boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)
# Plot variable importance
#plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")

```

Our utilization of the **Boruta** function for feature ranking and selection indicate that:

* `Credit_History`, `TotalIncome`, `LoanAmount`, and `Self_Employed` are strong predictors,
* `Property_Area` is a moderate predictor, and
* `Married`, `Loan_Amount_Term`, `Education`, `Gender`, and `Dependents` are all poor predictors.

With feature importance in mind, we drop the poor predictors from consideration. Dropping these variables also addresses concerns of applicant discrimination (ie. rejection based on `Gender`) and thus we address two concerns with this act of feature selection.

```{r, include=F}
#Subset data based on predictor strength
loan <- subset(loan, select = -c(1, 2, 3, 4, 7) ) #drop poor predictors
#head(loan) #verify

```

## Independent Variable Distributions

With our loan dataset properly subset, we proceed to observing the distributions of our independent variables. First we observe numeric distributions:

```{r}
#convert CreditHistory to type factor
loan$Credit_History <- factor(loan$Credit_History)
#levels(loan$Credit_History) #verify

#Numeric distributions
loan %>%
    keep(is.numeric) %>%
    gather() %>% 
    ggplot(aes(value)) +
        facet_wrap(~ key, scales = "free", ncol=1) +
        geom_histogram(bins=90,color="darkblue", fill="lightblue")

```

From the above figures we observe that `LoanAmount` and `TotalIncome` appear to be right skewed normal
and there are a number of noteworthy outliers for both distributions. From this, we note the importance of outlier-handling and scaling as critical steps in building our neural network model.

Next, we explore our categorical variables:

```{r}
#Categoric distributions
##Self_Employed
p1 <- loan %>% dplyr::select(1,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Self_Employed, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")
##Self_Employed
p2 <- loan %>% dplyr::select(3,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Credit_History, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")
##Property_Area
p3 <- loan %>% dplyr::select(4,5) %>% group_by(,Loan_Status) %>% count() %>%
    ggplot(aes(x=Property_Area, y=freq, fill=Loan_Status)) + 
        geom_bar(stat='identity', position="stack")

grid.arrange(p1, p2, p3, nrow = 2, ncol = 2)

```

From the above figures we can extend:

* non self employed outnumbers self employed on a 5:1 basis, 
* credit history meeting qualifications outnumbers not meeting qualifications on a 5:1 basis,
* properties in semiurban areas make up a slight majority, and
* with regard to loan approval, it appears that being self-employed, having a strong credit history, and living in a semiurban area are advantageous. *The strongest categorical predictor appears to be that the applicant have a credit history that meets qualifications.*

With a relatively thorough exploratory analysis under our belt, we move on to building our neural network model.

***


# Model Building

We'll utilize the *holdout-validation method* for evaluating model performance. We train-test split our data using a 75:25 partition, build our model on the training set and then evaluate its performance on the test set.

## Neural Network (baseline)

To start, we compute our "barrier value" and then partition our data based on this value, with 75% of our data going in the training set and 25% of our data going in the test set.

```{r}
set.seed(123) #for reproducibility

bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value

#Subset: train-test split based on partition value
train <- loan[partition, ] 
test <- loan[-partition, ]

#print(dim(train)) #460 x 6
#print(dim(test)) #154 x 6

```

We set our training algorithm’s parameters and then train our model using the train() function with "nnet" passed in as the method and "scale" and "center" passed in so that numeric variables are standardized.

```{r, include=F}
#Specify training algorithm parameters
train_params <- trainControl(method = "repeatedcv", number = 10, repeats=5)

#Train neural net model and standardize variables via preProcess method
nnet_model1 <- train(train[,-5], train$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)
```

With our "baseline model" trained, we proceed to model evaluation. We verify the baseline accuracy (0.676) and then evaluate our model's performance against this "baseline". We generate predictions based on the training set and then output these predictions as a confusion matrix and then we do the same with our test data.

```{r}
#round(prop.table(table(train$Loan_Status)),3)   #Baseline accuracy Y: 0.676

#Training predictions
nnPred_train <-predict(nnet_model1, train)
#Training confusion matrix
table(train$Loan_Status, nnPred_train)
round((308+78)/nrow(train),3)                    

#Test predictions
nnPred_test <-predict(nnet_model1, test)
#Test confusion matrix
table(test$Loan_Status, nnPred_test)
round((105+25)/nrow(test),3) 
```

From above, we observe a training accuracy of 83.9% and a test accuracy of 84.4% which is a marked improvement of nearly 20% over our "baseline accuracy".

By merely applying a neural network model to our dataset, we see major improvements in predictive capability. Next, we see if we can take the model further. If we can improve model performance by handling outliers and creating features prior to feeding the model.

## Outlier Handling and Feature Creation (optimizations)

We explore the affects of outlier handling and feature creation on model performance to determine if either step improves our model.

We start by re-visiting the distribution of outliers via boxplot:

```{r, fig.height=8, fig.width=8} 
#Observe the affect of outlier-handling on model performance (if any)

#Confirm the presence of influential observations
p4 <- ggplot(loan) +
  aes(x = Loan_Status, y = LoanAmount) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()

p5 <- ggplot(loan) +
  aes(x = Loan_Status, y = TotalIncome) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()

grid.arrange(p4, p5, nrow = 1, ncol = 2)

```

From above we can see that outliers appear to be a concern for our model. To rectify the situation, we identify the outliers using the boxplot.stats() function, filter for corresponding observations, remove outliers from our dataset, and revisit model performance.

```{r, include=F, eval=F}
#Identify outlier locations for LoanAmount, TotalIncome
out1 <- boxplot.stats(loan$LoanAmount)$out
outliers1 <- which(loan$LoanAmount %in% c(out1))

out2 <- boxplot.stats(loan$TotalIncome)$out
outliers2 <- which(loan$TotalIncome %in% c(out2))

outliers <- c(outliers1, outliers2) #merge lists
outliers <- unique(outliers) #remove repeat values

#Remove outliers
loan <- loan[-outliers,]

#Observe affect on model performance
set.seed(123) #for reproducibility

bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition2 <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value

#Subset: train-test split based on partition value
train2 <- loan[partition2, ] 
test2 <- loan[-partition2, ]

#Train neural net model and standardize variables via preProcess method
nnet_model2 <- train(train2[,-5], train2$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)

#Training predictions
nnPred_train2 <-predict(nnet_model2, train2)
#Training confusion matrix
table(train2$Loan_Status, nnPred_train2)
round((290+62)/nrow(train2),3) #0.856 - LOWER                  

#Test predictions
nnPred_test2 <-predict(nnet_model2, test2)
#Test confusion matrix
table(test2$Loan_Status, nnPred_test2)
round((85+24)/nrow(test),3) #0.708 - LOWER

```

Outlier-handling led to performance improvements on the training set (up to 85.6% accuracy) and reduction on the test set (down to 70.8%). As such we elected not to include outlier-handling as an optimization step. *Note: corresponding code in Appendix.*

We proceeded to observe the affect of feature creation on model performance. We wanted to see if adding certain combinations of features would improve our predictive accuracy. We tested the inclusion of variables for:

* self employed with high income,
* semiurban property with qualified credit history,
* not self employed with low loan amount, and
* low income with high loan amount.

The inclusion of these variables, and feature creation in general, *slightly* reduced the performance of our model and so we elected to exclude it as a modeling optimization step. *Note: corresponding code in Appendix.*

```{r, include=F, eval=F}
#Observe the affect of feature creation on model performance (if any)

#if self employed and income greater than
loan$hiINC_SE <- as.factor(ifelse(loan$TotalIncome >= 7522 & loan$Self_Employed == "Yes", 1, 0))

#if semiurban property and credit history 1
loan$SEMI_CH <- as.factor(ifelse(loan$Property_Area == "Semiurban" & loan$Credit_History == 1, 1, 0))

#if not self employed and loan amount below
loan$notSE_CH <- as.factor(ifelse(loan$Self_Employed == "No" & loan$LoanAmount <= 100.0, 1, 0))

#if income below and loan amount above
loan$loINC_hiAMT <- as.factor(ifelse(loan$TotalIncome <= 4166 & loan$LoanAmount >= 166.8, 1, 0))

#head(loan) #verify

```

```{r, include=F, eval=F}
set.seed(123) #for reproducibility

bar <- floor(0.75 * nrow(loan)) #compute "barrier value"
partition2 <- sample(seq_len(nrow(loan)), size = bar) #sample based on barrier value

#Subset: train-test split based on partition value
train2 <- loan[partition2, ] 
test2 <- loan[-partition2, ]

#Train neural net model and standardize variables via preProcess method
nnet_model2 <- train(train2[,-5], train2$Loan_Status,
                 method = "nnet",
                 trControl= train_params,
                 preProcess=c("scale","center"),
                 na.action = na.omit
)

#Training predictions
nnPred_train2 <-predict(nnet_model2, train2)
#Training confusion matrix
table(train2$Loan_Status, nnPred_train2)
round((306+71)/nrow(train2),3) #0.82 - LOWER                  

#Test predictions
nnPred_test2 <-predict(nnet_model2, test2)
#Test confusion matrix
table(test2$Loan_Status, nnPred_test2)
round((106+23)/nrow(test),3) #0.838 - LOWER

```


```{r}
#Re-assess the model


```


## Alternative NN Approach (optional)

## Model Selection

## Final Model Discussion / Interpretation

***

# Model Selection / Comparison

For this section we could re-include HW3 models improved **based on the Prof's feedback**.

1. KNN: which k-value would we actually pick? Could re-apply to loan data (rather than penguin).
2. DT: should be able to handle categorical variables | explore a longer tree | show variable importance factor
3. RF: derive a different variable (ie. combined applicant income and co-applicant income as total income). Speak in greater depth regarding higher class error in predicting 'no' and then discuss what this models strengths and weaknesses may be (ie. predicting 'no' class).

Based on the Professor's HW3 feedback, we could improve associated code chunks in the following manner. This is more a "nice to have" than any sortof requirement. It could be nice to show the Prof that we listened to her feedback before re-including this code to compare the performance of our NN model to ...

## Comparison Table

***

# Conclusion

## Findings

## Next Steps

***

# References

* https://www.youtube.com/watch?v=bfmFfD2RIcg [neural net background]
* https://www.ibm.com/cloud/learn/neural-networks [neural net background]
* https://www.pluralsight.com/guides/machine-learning-with-neural-networks-r [neural net model]

***

# Appendices
